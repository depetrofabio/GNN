{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Knowledge Graph Embeddings\n",
    "\n",
    "This notebook covers methods to **embed Knowledge Graphs (KGs)** — learning low-dimensional vector representations of entities and relations — and an introduction to **Heterogeneous Graphs**.\n",
    "\n",
    "| Model | Paper | Key Idea |\n",
    "|-------|-------|----------|\n",
    "| **TransE** | Bordes et al. (2013) | $h + r \\approx t$ in Euclidean space |\n",
    "| **TransR** | Lin et al. (2015) | Entity/relation in separate spaces, projection matrix |\n",
    "| **RotatE** | Sun et al. (2019) | Relation as rotation in complex space |\n",
    "| **ComplEx** | Trouillon et al. (2016) | Complex-valued embeddings for asymmetric relations |\n",
    "| **LiteralE** | Kristiansen et al. (2018) | Incorporate literal attributes into embeddings |\n",
    "| **Heterogeneous Graphs** | — | Multiple node/edge types |\n",
    "\n",
    "**Contents**\n",
    "1. [Knowledge Graphs: Background](#1-knowledge-graphs-background)\n",
    "2. [TransE](#2-transe)\n",
    "3. [TransR](#3-transr)\n",
    "4. [RotatE](#4-rotate)\n",
    "5. [ComplEx](#5-complex)\n",
    "6. [LiteralE](#6-literale)\n",
    "7. [Heterogeneous Graphs](#7-heterogeneous-graphs)\n",
    "8. [Exercises](#8-exercises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install\n",
    "# !pip install torch torch_geometric\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "\n",
    "try:\n",
    "    import torch_geometric\n",
    "    print(f'PyG version: {torch_geometric.__version__}')\n",
    "    pyg_available = True\n",
    "except ImportError:\n",
    "    print('PyTorch Geometric not installed')\n",
    "    pyg_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Knowledge Graphs: Background\n",
    "\n",
    "### 1.1 What is a Knowledge Graph?\n",
    "\n",
    "A **Knowledge Graph (KG)** is a multi-relational directed graph $\\mathcal{G} = (\\mathcal{E}, \\mathcal{R}, \\mathcal{T})$ where:\n",
    "- $\\mathcal{E}$ — set of **entities** (nodes)\n",
    "- $\\mathcal{R}$ — set of **relation types** (edge labels)\n",
    "- $\\mathcal{T} \\subseteq \\mathcal{E} \\times \\mathcal{R} \\times \\mathcal{E}$ — set of **triples** $(h, r, t)$ meaning \"head entity $h$ has relation $r$ to tail entity $t$\"\n",
    "\n",
    "**Example triples:**\n",
    "```\n",
    "(London, capitalOf, UK)\n",
    "(UK, locatedIn, Europe)\n",
    "(Shakespeare, bornIn, StratfordUponAvon)\n",
    "```\n",
    "\n",
    "### 1.2 Knowledge Graph Completion (Link Prediction)\n",
    "\n",
    "Real-world KGs are **incomplete**. The task of **KG completion** is to predict missing triples, i.e.:\n",
    "- Given $(h, r, ?)$ → predict the most likely tail entity\n",
    "- Given $(?, r, t)$ → predict the most likely head entity\n",
    "\n",
    "### 1.3 KG Embedding Approach\n",
    "\n",
    "All KG embedding methods share a common framework:\n",
    "1. Assign each entity $e \\in \\mathcal{E}$ a vector $\\mathbf{e} \\in \\mathbb{R}^d$ (or $\\mathbb{C}^d$)\n",
    "2. Assign each relation $r \\in \\mathcal{R}$ a vector (or matrix) $\\mathbf{r}$\n",
    "3. Define a **scoring function** $f(h, r, t) \\in \\mathbb{R}$ that should be high for true triples and low for false ones\n",
    "4. Train with a margin-based or binary cross-entropy loss\n",
    "\n",
    "### 1.4 Evaluation Metrics\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| **MRR** | Mean Reciprocal Rank: $\\frac{1}{|\\mathcal{T}|}\\sum_{i} \\frac{1}{\\text{rank}_i}$ |\n",
    "| **Hits@k** | Fraction of test triples where the correct entity is in top-$k$ |\n",
    "| **MR** | Mean Rank |\n",
    "\n",
    "### 1.5 Toy Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy KG with 6 entities and 3 relations\n",
    "entity2id = {\n",
    "    'Alice': 0, 'Bob': 1, 'Carol': 2,\n",
    "    'London': 3, 'Paris': 4, 'UK': 5\n",
    "}\n",
    "relation2id = {\n",
    "    'knows': 0, 'livesIn': 1, 'locatedIn': 2\n",
    "}\n",
    "\n",
    "# Triples as (head_id, relation_id, tail_id)\n",
    "triples = [\n",
    "    (0, 0, 1),  # Alice knows Bob\n",
    "    (1, 0, 2),  # Bob knows Carol\n",
    "    (0, 1, 3),  # Alice livesIn London\n",
    "    (1, 1, 4),  # Bob livesIn Paris\n",
    "    (3, 2, 5),  # London locatedIn UK\n",
    "]\n",
    "\n",
    "num_entities = len(entity2id)\n",
    "num_relations = len(relation2id)\n",
    "\n",
    "triples_tensor = torch.tensor(triples, dtype=torch.long)  # [T, 3]\n",
    "print('Entities:', num_entities)\n",
    "print('Relations:', num_relations)\n",
    "print('Training triples:', triples_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the toy KG\n",
    "try:\n",
    "    import networkx as nx\n",
    "\n",
    "    id2entity = {v: k for k, v in entity2id.items()}\n",
    "    id2relation = {v: k for k, v in relation2id.items()}\n",
    "\n",
    "    DG = nx.MultiDiGraph()\n",
    "    for h, r, t in triples:\n",
    "        DG.add_edge(id2entity[h], id2entity[t], label=id2relation[r])\n",
    "\n",
    "    pos = nx.spring_layout(DG, seed=7)\n",
    "    nx.draw_networkx_nodes(DG, pos, node_color='lightblue', node_size=1500)\n",
    "    nx.draw_networkx_labels(DG, pos, font_size=9)\n",
    "    edge_labels = {(id2entity[h], id2entity[t]): id2relation[r] for h, r, t in triples}\n",
    "    nx.draw_networkx_edge_labels(DG, pos, edge_labels=edge_labels, font_size=8)\n",
    "    nx.draw_networkx_edges(DG, pos, arrows=True)\n",
    "    plt.title('Toy Knowledge Graph'); plt.axis('off'); plt.show()\n",
    "except ImportError:\n",
    "    print('networkx not installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. TransE\n",
    "\n",
    "### 2.1 Theory\n",
    "\n",
    "**TransE** (Bordes et al., 2013) is the seminal translational distance model. It models a relation $r$ as a **translation** in embedding space:\n",
    "\n",
    "$$\\mathbf{h} + \\mathbf{r} \\approx \\mathbf{t}$$\n",
    "\n",
    "#### Scoring Function\n",
    "\n",
    "$$f(h, r, t) = -\\|\\mathbf{h} + \\mathbf{r} - \\mathbf{t}\\|_{1/2}$$\n",
    "\n",
    "(Negative $L_1$ or $L_2$ distance — higher score = more plausible triple)\n",
    "\n",
    "#### Training Loss — Margin-Based\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{(h,r,t) \\in \\mathcal{T}} \\sum_{(h',r,t') \\in \\mathcal{T}^-}\n",
    "\\left[\\gamma + f(h',r,t') - f(h,r,t)\\right]_+$$\n",
    "\n",
    "where $\\gamma > 0$ is the margin, $[x]_+ = \\max(0, x)$, and $\\mathcal{T}^-$ contains **negative** (corrupted) triples.\n",
    "\n",
    "#### Negative Sampling\n",
    "\n",
    "Negative triples are created by randomly replacing either $h$ or $t$ with a random entity:\n",
    "$$\\mathcal{T}^- = \\{(h', r, t) : h' \\in \\mathcal{E}\\} \\cup \\{(h, r, t') : t' \\in \\mathcal{E}\\}$$\n",
    "\n",
    "#### Limitations\n",
    "- Cannot model **1-to-N**, **N-to-1**, and **N-to-N** relations (forces entities to a single point)\n",
    "- Assumes every relation has a unique inverse (cannot model symmetric relations well)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 TransE Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransE(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim, margin=1.0, norm=1):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        self.norm = norm\n",
    "\n",
    "        self.entity_emb  = nn.Embedding(num_entities,  embedding_dim)\n",
    "        self.relation_emb = nn.Embedding(num_relations, embedding_dim)\n",
    "\n",
    "        # Initialise uniformly\n",
    "        nn.init.uniform_(self.entity_emb.weight,  -6/embedding_dim**0.5, 6/embedding_dim**0.5)\n",
    "        nn.init.uniform_(self.relation_emb.weight, -6/embedding_dim**0.5, 6/embedding_dim**0.5)\n",
    "\n",
    "        # Normalise relation embeddings to unit norm\n",
    "        with torch.no_grad():\n",
    "            self.relation_emb.weight.data = F.normalize(self.relation_emb.weight.data, p=2, dim=1)\n",
    "\n",
    "    def score(self, h_idx, r_idx, t_idx):\n",
    "        \"\"\"Lower score = more plausible triple.\"\"\"\n",
    "        h = F.normalize(self.entity_emb(h_idx), p=2, dim=1)\n",
    "        r = self.relation_emb(r_idx)\n",
    "        t = F.normalize(self.entity_emb(t_idx), p=2, dim=1)\n",
    "        return torch.norm(h + r - t, p=self.norm, dim=1)\n",
    "\n",
    "    def forward(self, pos_triples, neg_triples):\n",
    "        \"\"\"Margin-ranking loss.\"\"\"\n",
    "        ph, pr, pt = pos_triples[:, 0], pos_triples[:, 1], pos_triples[:, 2]\n",
    "        nh, nr, nt = neg_triples[:, 0], neg_triples[:, 1], neg_triples[:, 2]\n",
    "\n",
    "        pos_score = self.score(ph, pr, pt)\n",
    "        neg_score = self.score(nh, nr, nt)\n",
    "\n",
    "        loss = F.relu(self.margin + pos_score - neg_score).mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "def corrupt_triples(triples, num_entities):\n",
    "    \"\"\"Generate negative triples by randomly replacing head or tail.\"\"\"\n",
    "    neg = triples.clone()\n",
    "    mask = torch.rand(len(triples)) > 0.5   # 50% chance replace head, 50% tail\n",
    "    random_entities = torch.randint(0, num_entities, (len(triples),))\n",
    "    neg[mask, 0]  = random_entities[mask]    # replace head\n",
    "    neg[~mask, 2] = random_entities[~mask]   # replace tail\n",
    "    return neg\n",
    "\n",
    "\n",
    "print('TransE defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train TransE on the toy KG\n",
    "dim = 10\n",
    "model_transe = TransE(num_entities, num_relations, dim, margin=1.0)\n",
    "optimizer = optim.Adam(model_transe.parameters(), lr=0.01)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(500):\n",
    "    neg_triples = corrupt_triples(triples_tensor, num_entities)\n",
    "    optimizer.zero_grad()\n",
    "    loss = model_transe(triples_tensor, neg_triples)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch'); plt.ylabel('Margin Loss')\n",
    "plt.title('TransE Training Loss'); plt.show()\n",
    "print(f'Final loss: {losses[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect learned embeddings\n",
    "model_transe.eval()\n",
    "with torch.no_grad():\n",
    "    ent_embs = F.normalize(model_transe.entity_emb.weight, p=2, dim=1)\n",
    "    rel_embs = model_transe.relation_emb.weight\n",
    "\n",
    "id2entity = {v: k for k, v in entity2id.items()}\n",
    "\n",
    "# Verify: h + r ≈ t for (Alice, livesIn, London)\n",
    "alice_id = entity2id['Alice']\n",
    "london_id = entity2id['London']\n",
    "livesIn_id = relation2id['livesIn']\n",
    "\n",
    "pred = ent_embs[alice_id] + rel_embs[livesIn_id]\n",
    "dists = torch.norm(ent_embs - pred.unsqueeze(0), p=2, dim=1)\n",
    "ranked = dists.argsort()\n",
    "\n",
    "print('Query: (Alice, livesIn, ?)')\n",
    "for rank, idx in enumerate(ranked):\n",
    "    print(f'  Rank {rank+1}: {id2entity[idx.item()]} (dist={dists[idx].item():.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. TransR\n",
    "\n",
    "### 3.1 Theory\n",
    "\n",
    "**TransR** (Lin et al., 2015) addresses TransE's inability to handle complex relation patterns by introducing **relation-specific projection spaces**.\n",
    "\n",
    "**Key idea:** Entities and relations live in *different* spaces. Entities are projected into the relation space before the translation:\n",
    "\n",
    "$$\\mathbf{h}_r = \\mathbf{h}\\mathbf{M}_r, \\quad \\mathbf{t}_r = \\mathbf{t}\\mathbf{M}_r$$\n",
    "\n",
    "where $\\mathbf{M}_r \\in \\mathbb{R}^{d_e \\times d_r}$ is a relation-specific projection matrix.\n",
    "\n",
    "#### Scoring Function\n",
    "\n",
    "$$f(h, r, t) = -\\|\\mathbf{h}_r + \\mathbf{r} - \\mathbf{t}_r\\|_2^2$$\n",
    "\n",
    "#### Advantages over TransE\n",
    "- Different relations can model different geometric structures (e.g., a plane for 1-to-N)\n",
    "- Entity dimension $d_e$ and relation dimension $d_r$ can differ\n",
    "\n",
    "#### Disadvantage\n",
    "- $|\\mathcal{R}|$ projection matrices → large memory footprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 TransR Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransR(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, entity_dim, relation_dim, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        self.entity_dim = entity_dim\n",
    "        self.relation_dim = relation_dim\n",
    "\n",
    "        self.entity_emb  = nn.Embedding(num_entities,  entity_dim)\n",
    "        self.relation_emb = nn.Embedding(num_relations, relation_dim)\n",
    "        # Projection matrix for each relation: [num_relations, entity_dim * relation_dim]\n",
    "        self.proj_matrix = nn.Embedding(num_relations, entity_dim * relation_dim)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.entity_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.relation_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.proj_matrix.weight)\n",
    "\n",
    "    def project(self, entity_emb, proj):\n",
    "        \"\"\"Project entity embedding into relation space.\"\"\"\n",
    "        # entity_emb: [B, entity_dim]\n",
    "        # proj: [B, entity_dim * relation_dim]\n",
    "        B = entity_emb.size(0)\n",
    "        M = proj.view(B, self.entity_dim, self.relation_dim)\n",
    "        # [B, 1, entity_dim] @ [B, entity_dim, relation_dim] -> [B, 1, relation_dim]\n",
    "        projected = torch.bmm(entity_emb.unsqueeze(1), M).squeeze(1)\n",
    "        return F.normalize(projected, p=2, dim=1)\n",
    "\n",
    "    def score(self, h_idx, r_idx, t_idx):\n",
    "        h   = F.normalize(self.entity_emb(h_idx),  p=2, dim=1)\n",
    "        t   = F.normalize(self.entity_emb(t_idx),  p=2, dim=1)\n",
    "        r   = self.relation_emb(r_idx)\n",
    "        M_r = self.proj_matrix(r_idx)\n",
    "\n",
    "        h_r = self.project(h, M_r)\n",
    "        t_r = self.project(t, M_r)\n",
    "\n",
    "        return torch.norm(h_r + r - t_r, p=2, dim=1)\n",
    "\n",
    "    def forward(self, pos_triples, neg_triples):\n",
    "        ph, pr, pt = pos_triples[:, 0], pos_triples[:, 1], pos_triples[:, 2]\n",
    "        nh, nr, nt = neg_triples[:, 0], neg_triples[:, 1], neg_triples[:, 2]\n",
    "        pos_score = self.score(ph, pr, pt)\n",
    "        neg_score = self.score(nh, nr, nt)\n",
    "        return F.relu(self.margin + pos_score - neg_score).mean()\n",
    "\n",
    "\n",
    "model_transr = TransR(num_entities, num_relations, entity_dim=10, relation_dim=6)\n",
    "optimizer_r = optim.Adam(model_transr.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(500):\n",
    "    neg = corrupt_triples(triples_tensor, num_entities)\n",
    "    optimizer_r.zero_grad()\n",
    "    loss = model_transr(triples_tensor, neg)\n",
    "    loss.backward()\n",
    "    optimizer_r.step()\n",
    "\n",
    "print(f'TransR final loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. RotatE\n",
    "\n",
    "### 4.1 Theory\n",
    "\n",
    "**RotatE** (Sun et al., 2019) models each relation as a **rotation in complex space**.\n",
    "\n",
    "Entities are represented as complex vectors $\\mathbf{h}, \\mathbf{t} \\in \\mathbb{C}^d$ and each relation as a complex vector $\\mathbf{r} \\in \\mathbb{C}^d$ with $|r_k| = 1$ for all $k$:\n",
    "\n",
    "$$\\mathbf{h} \\circ \\mathbf{r} \\approx \\mathbf{t}$$\n",
    "\n",
    "where $\\circ$ denotes element-wise (Hadamard) product in $\\mathbb{C}^d$.\n",
    "\n",
    "Because $|r_k| = 1$, each dimension performs a **rotation by angle $\\theta_{r,k}$**:\n",
    "$$r_k = e^{i\\theta_{r,k}} = \\cos\\theta_{r,k} + i\\sin\\theta_{r,k}$$\n",
    "\n",
    "#### Scoring Function\n",
    "\n",
    "$$f(h, r, t) = -\\|\\mathbf{h} \\circ \\mathbf{r} - \\mathbf{t}\\|$$\n",
    "\n",
    "#### Relation Patterns Captured\n",
    "\n",
    "| Pattern | Example | How RotatE handles it |\n",
    "|---------|---------|---------------------|\n",
    "| **Symmetry** | *isSiblingOf* | $\\theta_r = 0$ or $\\pi$ |\n",
    "| **Antisymmetry** | *isParentOf* | $\\theta_r \\neq 0, \\pi$ |\n",
    "| **Inversion** | *isChildOf* = inverse of *isParentOf* | $\\theta_{r^{-1}} = -\\theta_r$ |\n",
    "| **Composition** | $r_1 \\circ r_2 = r_3$ | $\\theta_{r_3} = \\theta_{r_1} + \\theta_{r_2}$ |\n",
    "\n",
    "#### Self-Adversarial Negative Sampling\n",
    "\n",
    "RotatE uses a **self-adversarial** loss:\n",
    "$$\\mathcal{L} = -\\log\\sigma(\\gamma - d_r(h,t)) - \\sum_i p(h_i', r, t_i') \\log\\sigma(d_r(h_i', t_i') - \\gamma)$$\n",
    "\n",
    "where $p(h', r, t') \\propto \\exp(\\alpha \\cdot f(h', r, t'))$ is the probability of sampling negative triple $(h', r, t')$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 RotatE Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotatE(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim, margin=6.0):\n",
    "        super().__init__()\n",
    "        assert embedding_dim % 2 == 0, 'embedding_dim must be even (real + imag)'\n",
    "        self.dim = embedding_dim // 2  # complex dimension\n",
    "        self.margin = margin\n",
    "\n",
    "        # Store real and imaginary parts separately\n",
    "        self.entity_emb_re = nn.Embedding(num_entities, self.dim)\n",
    "        self.entity_emb_im = nn.Embedding(num_entities, self.dim)\n",
    "\n",
    "        # Only angle for relations (|r| = 1 enforced by using cos/sin)\n",
    "        self.relation_phase = nn.Embedding(num_relations, self.dim)\n",
    "\n",
    "        range_init = (6 / self.dim) ** 0.5\n",
    "        nn.init.uniform_(self.entity_emb_re.weight, -range_init, range_init)\n",
    "        nn.init.uniform_(self.entity_emb_im.weight, -range_init, range_init)\n",
    "        nn.init.uniform_(self.relation_phase.weight, -torch.pi, torch.pi)\n",
    "\n",
    "    def score(self, h_idx, r_idx, t_idx):\n",
    "        h_re = self.entity_emb_re(h_idx)\n",
    "        h_im = self.entity_emb_im(h_idx)\n",
    "        t_re = self.entity_emb_re(t_idx)\n",
    "        t_im = self.entity_emb_im(t_idx)\n",
    "\n",
    "        # Relation as unit complex: r = cos(θ) + i sin(θ)\n",
    "        theta = self.relation_phase(r_idx)\n",
    "        r_re = torch.cos(theta)\n",
    "        r_im = torch.sin(theta)\n",
    "\n",
    "        # h ○ r = (h_re * r_re - h_im * r_im) + i(h_re * r_im + h_im * r_re)\n",
    "        hr_re = h_re * r_re - h_im * r_im\n",
    "        hr_im = h_re * r_im + h_im * r_re\n",
    "\n",
    "        # Distance: ||h ○ r - t||\n",
    "        diff_re = hr_re - t_re\n",
    "        diff_im = hr_im - t_im\n",
    "        return torch.sqrt((diff_re ** 2 + diff_im ** 2).sum(dim=1) + 1e-8)\n",
    "\n",
    "    def forward(self, pos_triples, neg_triples):\n",
    "        ph, pr, pt = pos_triples[:, 0], pos_triples[:, 1], pos_triples[:, 2]\n",
    "        nh, nr, nt = neg_triples[:, 0], neg_triples[:, 1], neg_triples[:, 2]\n",
    "        pos_score = self.score(ph, pr, pt)\n",
    "        neg_score = self.score(nh, nr, nt)\n",
    "        return F.relu(self.margin + pos_score - neg_score).mean()\n",
    "\n",
    "\n",
    "model_rotate = RotatE(num_entities, num_relations, embedding_dim=10, margin=1.0)\n",
    "optimizer_rot = optim.Adam(model_rotate.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(500):\n",
    "    neg = corrupt_triples(triples_tensor, num_entities)\n",
    "    optimizer_rot.zero_grad()\n",
    "    loss = model_rotate(triples_tensor, neg)\n",
    "    loss.backward()\n",
    "    optimizer_rot.step()\n",
    "\n",
    "print(f'RotatE final loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visualising RotatE Relation Phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rotate.eval()\n",
    "with torch.no_grad():\n",
    "    phases = model_rotate.relation_phase.weight.cpu()  # [num_relations, dim]\n",
    "\n",
    "id2relation = {v: k for k, v in relation2id.items()}\n",
    "\n",
    "fig, axes = plt.subplots(1, num_relations, figsize=(4 * num_relations, 4))\n",
    "for r_id in range(num_relations):\n",
    "    theta = phases[r_id].numpy()  # angles\n",
    "    x = np.cos(theta)\n",
    "    y = np.sin(theta)\n",
    "    ax = axes[r_id]\n",
    "    circle = plt.Circle((0, 0), 1, color='lightgray', fill=False)\n",
    "    ax.add_patch(circle)\n",
    "    ax.scatter(x, y, s=50)\n",
    "    for i, (xi, yi) in enumerate(zip(x, y)):\n",
    "        ax.annotate(str(i), (xi, yi), fontsize=7)\n",
    "    ax.set_xlim(-1.3, 1.3); ax.set_ylim(-1.3, 1.3)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(f'Relation: {id2relation[r_id]}')\n",
    "\n",
    "plt.suptitle('RotatE: Rotation angles per relation dimension')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. ComplEx\n",
    "\n",
    "### 5.1 Theory\n",
    "\n",
    "**ComplEx** (Trouillon et al., 2016) uses **complex-valued** embeddings but with a different scoring function based on the **Hermitian inner product**.\n",
    "\n",
    "All entities and relations are embedded in $\\mathbb{C}^d$:\n",
    "$$\\mathbf{h}, \\mathbf{r}, \\mathbf{t} \\in \\mathbb{C}^d$$\n",
    "\n",
    "#### Scoring Function\n",
    "\n",
    "$$f(h, r, t) = \\text{Re}(\\langle \\mathbf{h}, \\mathbf{r}, \\bar{\\mathbf{t}} \\rangle) = \\text{Re}\\!\\left(\\sum_k h_k \\cdot r_k \\cdot \\overline{t_k}\\right)$$\n",
    "\n",
    "where $\\bar{\\mathbf{t}}$ is the **complex conjugate** of $\\mathbf{t}$.\n",
    "\n",
    "#### Why Complex Numbers?\n",
    "\n",
    "The Hermitian inner product is **asymmetric**: $\\langle h, r, \\bar{t} \\rangle \\neq \\langle t, r, \\bar{h} \\rangle$ in general, which allows modelling **asymmetric** and **antisymmetric** relations.\n",
    "\n",
    "#### Connection to Other Models\n",
    "\n",
    "ComplEx can be written in real coordinates:\n",
    "\n",
    "$$f(h, r, t) = \\mathbf{h}_{re}^\\top \\text{diag}(\\mathbf{r}_{re}) \\mathbf{t}_{re}\n",
    "+ \\mathbf{h}_{re}^\\top \\text{diag}(\\mathbf{r}_{im}) \\mathbf{t}_{im}\n",
    "+ \\mathbf{h}_{im}^\\top \\text{diag}(\\mathbf{r}_{re}) \\mathbf{t}_{im}\n",
    "- \\mathbf{h}_{im}^\\top \\text{diag}(\\mathbf{r}_{im}) \\mathbf{t}_{re}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 ComplEx Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplEx(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim, reg_lambda=1e-3):\n",
    "        super().__init__()\n",
    "        self.reg_lambda = reg_lambda\n",
    "\n",
    "        # Real and imaginary parts\n",
    "        self.ent_re  = nn.Embedding(num_entities,  embedding_dim)\n",
    "        self.ent_im  = nn.Embedding(num_entities,  embedding_dim)\n",
    "        self.rel_re  = nn.Embedding(num_relations, embedding_dim)\n",
    "        self.rel_im  = nn.Embedding(num_relations, embedding_dim)\n",
    "\n",
    "        for emb in [self.ent_re, self.ent_im, self.rel_re, self.rel_im]:\n",
    "            nn.init.xavier_uniform_(emb.weight)\n",
    "\n",
    "    def score(self, h_idx, r_idx, t_idx):\n",
    "        h_re = self.ent_re(h_idx);  h_im = self.ent_im(h_idx)\n",
    "        r_re = self.rel_re(r_idx);  r_im = self.rel_im(r_idx)\n",
    "        t_re = self.ent_re(t_idx);  t_im = self.ent_im(t_idx)\n",
    "\n",
    "        # Re(<h, r, conj(t)>)\n",
    "        return (\n",
    "            (h_re * r_re * t_re).sum(dim=1)\n",
    "          + (h_re * r_im * t_im).sum(dim=1)\n",
    "          + (h_im * r_re * t_im).sum(dim=1)\n",
    "          - (h_im * r_im * t_re).sum(dim=1)\n",
    "        )\n",
    "\n",
    "    def regularisation(self, h_idx, r_idx, t_idx):\n",
    "        # L3 regularisation on entity and relation embeddings\n",
    "        reg = (\n",
    "            self.ent_re(h_idx).norm(p=2, dim=1).pow(2).mean() +\n",
    "            self.ent_im(h_idx).norm(p=2, dim=1).pow(2).mean() +\n",
    "            self.rel_re(r_idx).norm(p=2, dim=1).pow(2).mean() +\n",
    "            self.rel_im(r_idx).norm(p=2, dim=1).pow(2).mean() +\n",
    "            self.ent_re(t_idx).norm(p=2, dim=1).pow(2).mean() +\n",
    "            self.ent_im(t_idx).norm(p=2, dim=1).pow(2).mean()\n",
    "        )\n",
    "        return self.reg_lambda * reg\n",
    "\n",
    "    def forward(self, triples, labels):\n",
    "        \"\"\"Binary cross-entropy loss with labels +1 / -1.\"\"\"\n",
    "        h_idx, r_idx, t_idx = triples[:, 0], triples[:, 1], triples[:, 2]\n",
    "        scores = self.score(h_idx, r_idx, t_idx)\n",
    "        # Sigmoid BCE\n",
    "        loss = F.softplus(-labels * scores).mean()\n",
    "        loss += self.regularisation(h_idx, r_idx, t_idx)\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Combine positive (+1) and negative (-1) triples\n",
    "neg_triples_init = corrupt_triples(triples_tensor, num_entities)\n",
    "all_triples  = torch.cat([triples_tensor, neg_triples_init], dim=0)\n",
    "all_labels   = torch.cat([\n",
    "    torch.ones(len(triples_tensor)),\n",
    "    -torch.ones(len(neg_triples_init))\n",
    "])\n",
    "\n",
    "model_complex = ComplEx(num_entities, num_relations, embedding_dim=10)\n",
    "optimizer_c   = optim.Adam(model_complex.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(500):\n",
    "    neg = corrupt_triples(triples_tensor, num_entities)\n",
    "    all_t = torch.cat([triples_tensor, neg], dim=0)\n",
    "    all_l = torch.cat([torch.ones(len(triples_tensor)), -torch.ones(len(neg))])\n",
    "    optimizer_c.zero_grad()\n",
    "    loss = model_complex(all_t, all_l)\n",
    "    loss.backward()\n",
    "    optimizer_c.step()\n",
    "\n",
    "print(f'ComplEx final loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. LiteralE\n",
    "\n",
    "### 6.1 Theory\n",
    "\n",
    "**LiteralE** (Kristiansen et al., 2018) extends KG embeddings to incorporate **literal attributes** (numerical or textual) associated with entities.\n",
    "\n",
    "#### Motivation\n",
    "\n",
    "Standard KG embeddings only use structural information (triples). However, KGs like Wikidata contain literals such as:\n",
    "```\n",
    "(Einstein, dateOfBirth, \"1879-03-14\")\n",
    "(Einstein, height, 1.75)\n",
    "```\n",
    "\n",
    "These literals provide valuable signals for link prediction.\n",
    "\n",
    "#### Core Idea\n",
    "\n",
    "LiteralE modifies entity embeddings by incorporating literal information via a **gate function** $g$:\n",
    "\n",
    "$$\\tilde{\\mathbf{e}} = g(\\mathbf{e}, \\mathbf{l}_e)$$\n",
    "\n",
    "where $\\mathbf{l}_e \\in \\mathbb{R}^L$ is a vector of literals for entity $e$ and $g$ is a learned transformation (e.g., linear gate):\n",
    "\n",
    "$$g(\\mathbf{e}, \\mathbf{l}_e) = \\sigma\\!\\left(\\mathbf{W}_1 \\mathbf{e} + \\mathbf{W}_2 \\mathbf{l}_e + \\mathbf{b}\\right)$$\n",
    "\n",
    "The enriched embedding $\\tilde{\\mathbf{e}}$ is then used in any base scoring function (e.g., DistMult, ComplEx).\n",
    "\n",
    "#### LiteralE variants\n",
    "- **LiteralE (DistMult)**: base model = DistMult\n",
    "- **LiteralE (ComplEx)**: base model = ComplEx\n",
    "- **Numerical gate** vs **Text gate** (for different literal types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 LiteralE Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiteralE(nn.Module):\n",
    "    \"\"\"LiteralE with DistMult as base model.\"\"\"\n",
    "\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim, num_literals):\n",
    "        super().__init__()\n",
    "        self.entity_emb  = nn.Embedding(num_entities,  embedding_dim)\n",
    "        self.relation_emb = nn.Embedding(num_relations, embedding_dim)\n",
    "\n",
    "        # Gate: transforms [entity_emb || literals] -> entity_dim\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + num_literals, embedding_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        nn.init.xavier_uniform_(self.entity_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.relation_emb.weight)\n",
    "\n",
    "    def enrich(self, entity_idx, literals):\n",
    "        \"\"\"Enrich entity embedding with literal information.\"\"\"\n",
    "        e = self.entity_emb(entity_idx)       # [B, d]\n",
    "        l = literals[entity_idx]              # [B, L]\n",
    "        combined = torch.cat([e, l], dim=-1)  # [B, d+L]\n",
    "        return self.gate(combined)             # [B, d]\n",
    "\n",
    "    def score(self, h_idx, r_idx, t_idx, literals):\n",
    "        \"\"\"DistMult scoring with enriched embeddings.\"\"\"\n",
    "        h_tilde = self.enrich(h_idx, literals)\n",
    "        t_tilde = self.enrich(t_idx, literals)\n",
    "        r = self.relation_emb(r_idx)\n",
    "        # DistMult: <h, r, t>\n",
    "        return (h_tilde * r * t_tilde).sum(dim=1)\n",
    "\n",
    "    def forward(self, triples, labels, literals):\n",
    "        h_idx, r_idx, t_idx = triples[:, 0], triples[:, 1], triples[:, 2]\n",
    "        scores = self.score(h_idx, r_idx, t_idx, literals)\n",
    "        return F.softplus(-labels * scores).mean()\n",
    "\n",
    "\n",
    "# Toy literals: 3 numerical attributes per entity (e.g., age, height, income)\n",
    "torch.manual_seed(0)\n",
    "literals = torch.randn(num_entities, 3)  # [num_entities, 3]\n",
    "\n",
    "model_literale = LiteralE(num_entities, num_relations,\n",
    "                           embedding_dim=10, num_literals=3)\n",
    "optimizer_le   = optim.Adam(model_literale.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(500):\n",
    "    neg = corrupt_triples(triples_tensor, num_entities)\n",
    "    all_t = torch.cat([triples_tensor, neg], dim=0)\n",
    "    all_l = torch.cat([torch.ones(len(triples_tensor)), -torch.ones(len(neg))])\n",
    "    optimizer_le.zero_grad()\n",
    "    loss = model_literale(all_t, all_l, literals)\n",
    "    loss.backward()\n",
    "    optimizer_le.step()\n",
    "\n",
    "print(f'LiteralE final loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Heterogeneous Graphs\n",
    "\n",
    "### 7.1 Theory\n",
    "\n",
    "A **Heterogeneous Graph** is a graph where nodes and/or edges can be of **multiple types**:\n",
    "\n",
    "$$G = (V, E, \\phi, \\psi)$$\n",
    "\n",
    "- $\\phi: V \\to \\mathcal{A}$ — node type mapping ($|\\mathcal{A}| > 1$)\n",
    "- $\\psi: E \\to \\mathcal{R}$ — edge (relation) type mapping ($|\\mathcal{R}| > 1$)\n",
    "\n",
    "**Example:** An academic network with:\n",
    "- Node types: `Author`, `Paper`, `Venue`\n",
    "- Edge types: `writes`, `cites`, `publishedIn`\n",
    "\n",
    "#### Meta-path\n",
    "\n",
    "A **meta-path** is a composite relation defined over a sequence of node and edge types:\n",
    "$$\\mathcal{P} = A_1 \\xrightarrow{R_1} A_2 \\xrightarrow{R_2} \\cdots \\xrightarrow{R_l} A_{l+1}$$\n",
    "\n",
    "Example: `Author → writes → Paper → cites → Paper` captures co-citation patterns.\n",
    "\n",
    "#### Why Heterogeneous GNNs?\n",
    "\n",
    "Standard GNNs treat all nodes/edges equally. Heterogeneous GNNs maintain **type-specific transformations**:\n",
    "$$\\mathbf{h}_v^{(l+1)} = \\text{AGG}\\Bigl(\\{\\mathbf{W}_{\\phi(v), \\psi(e)} \\mathbf{h}_u^{(l)} : u \\in \\mathcal{N}_{\\psi}(v)\\}\\Bigr)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Heterogeneous Graph with PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyg_available:\n",
    "    from torch_geometric.data import HeteroData\n",
    "\n",
    "    # Build a small academic heterogeneous graph\n",
    "    hdata = HeteroData()\n",
    "\n",
    "    # Node types\n",
    "    hdata['author'].x  = torch.randn(4, 8)   # 4 authors, 8 features\n",
    "    hdata['paper'].x   = torch.randn(6, 16)  # 6 papers, 16 features\n",
    "    hdata['venue'].x   = torch.randn(2, 4)   # 2 venues, 4 features\n",
    "\n",
    "    # Edge types: (src_type, edge_type, dst_type)\n",
    "    # author -writes-> paper\n",
    "    hdata['author', 'writes', 'paper'].edge_index = torch.tensor([\n",
    "        [0, 1, 2, 3, 0],\n",
    "        [0, 1, 2, 3, 4]\n",
    "    ])\n",
    "    # paper -cites-> paper\n",
    "    hdata['paper', 'cites', 'paper'].edge_index = torch.tensor([\n",
    "        [0, 1, 2, 3],\n",
    "        [1, 2, 3, 4]\n",
    "    ])\n",
    "    # paper -publishedIn-> venue\n",
    "    hdata['paper', 'publishedIn', 'venue'].edge_index = torch.tensor([\n",
    "        [0, 1, 2, 3, 4, 5],\n",
    "        [0, 0, 1, 1, 0, 1]\n",
    "    ])\n",
    "\n",
    "    print(hdata)\n",
    "    print('Node types:', hdata.node_types)\n",
    "    print('Edge types:', hdata.edge_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Heterogeneous GNN with `to_homogeneous` and `HeteroConv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyg_available:\n",
    "    from torch_geometric.nn import HeteroConv, SAGEConv as _SAGEConv\n",
    "\n",
    "    class HeteroGNN(nn.Module):\n",
    "        def __init__(self, hidden_channels, out_channels):\n",
    "            super().__init__()\n",
    "            # Project each node type to the same hidden dimension\n",
    "            self.proj = nn.ModuleDict({\n",
    "                'author': nn.Linear(8,  hidden_channels),\n",
    "                'paper' : nn.Linear(16, hidden_channels),\n",
    "                'venue' : nn.Linear(4,  hidden_channels),\n",
    "            })\n",
    "\n",
    "            # One SAGEConv per edge type\n",
    "            self.conv = HeteroConv({\n",
    "                ('author', 'writes',      'paper'): _SAGEConv(hidden_channels, hidden_channels),\n",
    "                ('paper',  'cites',       'paper'): _SAGEConv(hidden_channels, hidden_channels),\n",
    "                ('paper',  'publishedIn', 'venue'): _SAGEConv(hidden_channels, hidden_channels),\n",
    "            }, aggr='sum')\n",
    "\n",
    "            self.lin = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "        def forward(self, x_dict, edge_index_dict):\n",
    "            # Project all node types\n",
    "            x_dict = {ntype: F.relu(self.proj[ntype](x))\n",
    "                      for ntype, x in x_dict.items()}\n",
    "            # Heterogeneous convolution\n",
    "            x_dict = self.conv(x_dict, edge_index_dict)\n",
    "            x_dict = {k: F.relu(v) for k, v in x_dict.items()}\n",
    "            return x_dict\n",
    "\n",
    "\n",
    "    hetero_model = HeteroGNN(hidden_channels=32, out_channels=4)\n",
    "    out = hetero_model(hdata.x_dict, hdata.edge_index_dict)\n",
    "    for ntype, emb in out.items():\n",
    "        print(f'{ntype}: {emb.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 OGB-MAG Dataset (Large-Scale Heterogeneous Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: OGB datasets are large; this cell shows the API but does not download\n",
    "print(\"\"\"To load the OGB-MAG (Microsoft Academic Graph) dataset:\n",
    "\n",
    "    from torch_geometric.datasets import OGB_MAG\n",
    "    dataset = OGB_MAG(root='/tmp/mag', preprocess='metapath2vec')\n",
    "    data = dataset[0]\n",
    "    print(data)  # HeteroData with 4 node types and 4 edge types\n",
    "\n",
    "This heterogeneous graph has:\n",
    "  - 4 node types: paper, author, institution, field_of_study\n",
    "  - 4 edge types: cites, writes, affiliated_with, has_topic\n",
    "  - Task: node classification on papers into 349 classes\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Exercises\n",
    "\n",
    "### Exercise 1 — Link Prediction Evaluation\n",
    "\n",
    "Implement a **ranking-based evaluation** function:\n",
    "1. For each test triple $(h, r, t)$, compute scores for all possible tails: $\\{(h, r, t') : t' \\in \\mathcal{E}\\}$.\n",
    "2. Rank the correct tail $t$ among all candidates.\n",
    "3. Compute **MRR** and **Hits@1**, **Hits@3**, **Hits@10** for TransE, RotatE, and ComplEx on the toy KG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_kg(model, triples, num_entities, model_type='transe'):\n",
    "    \"\"\"Evaluate KG model via MRR and Hits@k.\"\"\"\n",
    "    model.eval()\n",
    "    ranks = []\n",
    "    with torch.no_grad():\n",
    "        for triple in triples:\n",
    "            h, r, t = triple\n",
    "            # TODO: score all possible tails, rank the correct one\n",
    "            ...\n",
    "    # TODO: compute MRR and Hits@k\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 — DistMult\n",
    "\n",
    "Implement **DistMult** (Yang et al., 2015):\n",
    "\n",
    "$$f(h, r, t) = \\mathbf{h}^\\top \\text{diag}(\\mathbf{r}) \\mathbf{t} = \\sum_k h_k r_k t_k$$\n",
    "\n",
    "Note: DistMult is a special case of ComplEx where all embeddings are real-valued. Compare its link prediction performance with TransE on the toy KG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 — your solution here\n",
    "class DistMult(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "        ...\n",
    "\n",
    "    def score(self, h_idx, r_idx, t_idx):\n",
    "        # TODO: <h, r, t>\n",
    "        ...\n",
    "\n",
    "    def forward(self, triples, labels):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 — Relation Pattern Analysis\n",
    "\n",
    "Using the RotatE model trained above:\n",
    "1. Find the angle $\\theta$ that best approximates the `knows` relation (should be close to 0 if symmetric).\n",
    "2. Check whether the relation `livesIn` has a different angle profile.\n",
    "3. Modify the training loop to explicitly include a **symmetric** triple pair: $(Alice, knows, Bob)$ and $(Bob, knows, Alice)$. Retrain and compare the angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 — your solution here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 — LiteralE with Text Features\n",
    "\n",
    "Extend the `LiteralE` implementation to support **text literals** by:\n",
    "1. Adding a `TextEncoder` module (e.g., a simple bag-of-words encoder or pre-computed TF-IDF vectors).\n",
    "2. Concatenating numerical and text literal vectors before the gate.\n",
    "3. Testing on the toy KG with added entity descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 — your solution here\n",
    "# Entity descriptions (simplified bag-of-words vectors)\n",
    "descriptions = {\n",
    "    'Alice': 'software engineer based in london',\n",
    "    'Bob':   'data scientist living in paris',\n",
    "    'Carol': 'researcher at university',\n",
    "    'London': 'capital city of the united kingdom',\n",
    "    'Paris':  'capital city of france',\n",
    "    'UK':     'country in northwestern europe',\n",
    "}\n",
    "# TODO: encode descriptions and extend LiteralE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 — Heterogeneous Link Prediction\n",
    "\n",
    "Using the academic heterogeneous graph:\n",
    "1. Add a **reverse edge type** for each edge type (e.g., `paper -written_by-> author`).\n",
    "2. Build a 2-layer `HeteroConv` model.\n",
    "3. Add a link prediction head: given a pair of `author` nodes, predict whether they collaborated (both wrote the same paper).\n",
    "4. Train with binary cross-entropy and report AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5 — your solution here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Model | Space | Scoring Function | Key Strength |\n",
    "|-------|-------|------------------|--------------|\n",
    "| **TransE** | $\\mathbb{R}^d$ | $-\\|\\mathbf{h}+\\mathbf{r}-\\mathbf{t}\\|$ | Simple, effective for 1-to-1 relations |\n",
    "| **TransR** | $\\mathbb{R}^d, \\mathbb{R}^k$ | $-\\|\\mathbf{h}_r+\\mathbf{r}-\\mathbf{t}_r\\|$ | Handles 1-to-N via projection |\n",
    "| **RotatE** | $\\mathbb{C}^d$ | $-\\|\\mathbf{h}\\circ\\mathbf{r}-\\mathbf{t}\\|$ | Captures symmetry, antisymmetry, inversion, composition |\n",
    "| **ComplEx** | $\\mathbb{C}^d$ | $\\text{Re}(\\langle\\mathbf{h},\\mathbf{r},\\bar{\\mathbf{t}}\\rangle)$ | Asymmetric relations, strong at large-scale |\n",
    "| **LiteralE** | $\\mathbb{R}^d$ | Any + gate | Incorporates entity attributes |\n",
    "| **Hetero Graphs** | Multi-type | Type-specific transforms | Multiple node/edge types |\n",
    "\n",
    "**Next notebook →** `04_advanced_gnns.ipynb` — Graph Transformer, Heterogeneous Graph Transformer, and R-GCN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
