{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: PyTorch and PyTorch Geometric\n",
    "\n",
    "This notebook introduces **PyTorch** — the foundational deep learning framework — and **PyTorch Geometric (PyG)** — the library built on top of PyTorch for deep learning on graphs and other irregular structures.\n",
    "\n",
    "**Contents**\n",
    "1. [PyTorch Fundamentals](#1-pytorch-fundamentals)  \n",
    "2. [Tensors and Autograd](#2-tensors-and-autograd)  \n",
    "3. [Building Neural Networks with `nn.Module`](#3-building-neural-networks-with-nnmodule)  \n",
    "4. [Introduction to Graph-Structured Data](#4-introduction-to-graph-structured-data)  \n",
    "5. [PyTorch Geometric: Core Concepts](#5-pytorch-geometric-core-concepts)  \n",
    "6. [Datasets and DataLoaders in PyG](#6-datasets-and-dataloaders-in-pyg)  \n",
    "7. [Your First GNN Layer (`GCNConv`)](#7-your-first-gnn-layer-gcnconv)  \n",
    "8. [Exercises](#8-exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install the required packages (run once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the lines below if you are in a fresh environment\n",
    "# !pip install torch torchvision\n",
    "# !pip install torch_geometric\n",
    "# Some PyG optional dependencies (speeds up sparse operations):\n",
    "# !pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. PyTorch Fundamentals\n",
    "\n",
    "### 1.1 What is PyTorch?\n",
    "\n",
    "**PyTorch** is an open-source deep learning framework developed by Meta AI. Its two main features are:\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Dynamic computation graphs** | Graphs are built on-the-fly during the forward pass, making debugging intuitive |\n",
    "| **Automatic differentiation** | The `autograd` engine computes gradients automatically |\n",
    "\n",
    "### 1.2 Core data structure: `torch.Tensor`\n",
    "\n",
    "A **Tensor** is a multi-dimensional array, similar to NumPy arrays but with GPU acceleration and gradient support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Tensors and Autograd\n",
    "\n",
    "### 2.1 Creating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalars, vectors, matrices, and higher-dimensional tensors\n",
    "scalar = torch.tensor(3.14)\n",
    "vector = torch.tensor([1.0, 2.0, 3.0])\n",
    "matrix = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "tensor_3d = torch.randn(2, 3, 4)  # random normal\n",
    "\n",
    "print('scalar:', scalar, '| shape:', scalar.shape)\n",
    "print('vector:', vector, '| shape:', vector.shape)\n",
    "print('matrix:\\n', matrix, '| shape:', matrix.shape)\n",
    "print('3D tensor shape:', tensor_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factory functions\n",
    "zeros = torch.zeros(3, 3)\n",
    "ones  = torch.ones(3, 3)\n",
    "eye   = torch.eye(3)          # identity matrix\n",
    "arange = torch.arange(0, 10, 2)  # [0, 2, 4, 6, 8]\n",
    "linspace = torch.linspace(0, 1, 5)  # [0, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "print('zeros:\\n', zeros)\n",
    "print('arange:', arange)\n",
    "print('linspace:', linspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1., 2.], [3., 4.]])\n",
    "b = torch.tensor([[5., 6.], [7., 8.]])\n",
    "\n",
    "print('Element-wise addition:\\n', a + b)\n",
    "print('Matrix multiplication:\\n', a @ b)       # or torch.matmul(a, b)\n",
    "print('Element-wise multiplication:\\n', a * b)\n",
    "print('Transpose:\\n', a.T)\n",
    "print('Sum of all elements:', a.sum())\n",
    "print('Mean along dim=0:', a.mean(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Reshaping Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(12, dtype=torch.float32)\n",
    "print('Original:', x.shape)         # (12,)\n",
    "\n",
    "x_2d = x.view(3, 4)\n",
    "print('After view(3,4):', x_2d.shape)   # (3, 4)\n",
    "\n",
    "x_3d = x.reshape(2, 2, 3)\n",
    "print('After reshape(2,2,3):', x_3d.shape)\n",
    "\n",
    "# squeeze / unsqueeze: add or remove dimensions of size 1\n",
    "y = torch.randn(1, 4, 1)\n",
    "print('y shape:', y.shape)\n",
    "print('squeeze:', y.squeeze().shape)         # (4,)\n",
    "print('unsqueeze(0):', y.squeeze().unsqueeze(0).shape)  # (1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Automatic Differentiation (`autograd`)\n",
    "\n",
    "PyTorch tracks operations on tensors with `requires_grad=True` and can automatically compute gradients via **backpropagation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example: y = 3x^2 + 2x + 1\n",
    "# dy/dx at x=2 should be 6x + 2 = 14\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = 3 * x**2 + 2 * x + 1\n",
    "y.backward()   # compute dy/dx\n",
    "print('dy/dx at x=2:', x.grad)   # expected: 14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector-valued function — needs a gradient vector (Jacobian-vector product)\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x ** 2\n",
    "y.backward(torch.ones_like(x))  # sum of gradients\n",
    "print('Gradients:', x.grad)     # [2, 4, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Simple Training Loop\n",
    "\n",
    "Let's fit a simple linear model $y = wx + b$ to random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate synthetic data: y = 2x + 1 + noise\n",
    "X = torch.randn(100, 1)\n",
    "y_true = 2 * X + 1 + 0.1 * torch.randn(100, 1)\n",
    "\n",
    "# Model parameters\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "lr = 0.01\n",
    "losses = []\n",
    "\n",
    "for epoch in range(300):\n",
    "    y_pred = X * w + b\n",
    "    loss = ((y_pred - y_true) ** 2).mean()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad\n",
    "        b -= lr * b.grad\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "print(f'Learned w={w.item():.4f}, b={b.item():.4f}  (true: w=2.0, b=1.0)')\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch'); plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Building Neural Networks with `nn.Module`\n",
    "\n",
    "PyTorch's `torch.nn.Module` is the base class for all neural network modules. A model is built by:\n",
    "1. Subclassing `nn.Module`\n",
    "2. Defining layers in `__init__`\n",
    "3. Implementing the forward pass in `forward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple Multi-Layer Perceptron.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, hidden_size, out_features):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "model = MLP(in_features=16, hidden_size=32, out_features=4)\n",
    "print(model)\n",
    "\n",
    "# Count trainable parameters\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Trainable parameters: {num_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with nn.Module, an optimizer, and a loss function\n",
    "torch.manual_seed(0)\n",
    "X_train = torch.randn(200, 16)\n",
    "y_train = torch.randint(0, 4, (200,))\n",
    "\n",
    "model = MLP(16, 32, 4)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(X_train)\n",
    "    loss = criterion(logits, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Final loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Introduction to Graph-Structured Data\n",
    "\n",
    "### 4.1 What is a Graph?\n",
    "\n",
    "A **graph** $G = (V, E)$ consists of:\n",
    "- $V$ — a set of **nodes** (vertices)\n",
    "- $E \\subseteq V \\times V$ — a set of **edges** connecting pairs of nodes\n",
    "\n",
    "Graphs can additionally carry:\n",
    "- **Node features** $\\mathbf{X} \\in \\mathbb{R}^{|V| \\times F}$ — feature vector for each node\n",
    "- **Edge features** — attributes on each edge\n",
    "- **Graph-level labels** — a label for the entire graph\n",
    "\n",
    "### 4.2 Graph Representations\n",
    "\n",
    "| Representation | Description | Memory |\n",
    "|---|---|---|\n",
    "| **Adjacency matrix** | $A \\in \\{0,1\\}^{N \\times N}$, $A_{ij}=1$ if edge $(i,j)$ exists | $O(N^2)$ |\n",
    "| **Edge list** | List of tuples $(i, j)$ | $O(\\|E\\|)$ |\n",
    "| **COO sparse format** | Two arrays `row` and `col` | $O(\\|E\\|)$ |\n",
    "\n",
    "PyG uses the **COO format** via `edge_index` of shape `[2, |E|]`.\n",
    "\n",
    "### 4.3 Why Standard NNs Fail on Graphs?\n",
    "\n",
    "Standard neural networks assume fixed-size, ordered inputs. Graphs have:\n",
    "- **Variable size** — different numbers of nodes and edges\n",
    "- **No canonical ordering** — node order is arbitrary (**permutation invariance**)\n",
    "- **Irregular connectivity** — each node can have a different number of neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual graph example\n",
    "# 5-node graph: 0--1--2--3, 1--4\n",
    "edge_index = torch.tensor([\n",
    "    [0, 1, 1, 2, 2, 3, 1, 4],   # source nodes\n",
    "    [1, 0, 2, 1, 3, 2, 4, 1],   # target nodes\n",
    "], dtype=torch.long)\n",
    "\n",
    "x = torch.randn(5, 4)  # 5 nodes, 4 features each\n",
    "print('edge_index shape:', edge_index.shape)  # [2, 8]\n",
    "print('node features shape:', x.shape)         # [5, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the graph with networkx\n",
    "try:\n",
    "    import networkx as nx\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(5))\n",
    "    edges = edge_index.T.tolist()\n",
    "    G.add_edges_from(edges)\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    nx.draw(G, pos, with_labels=True, node_color='lightblue',\n",
    "            node_size=700, font_size=12)\n",
    "    plt.title('Example graph'); plt.show()\n",
    "except ImportError:\n",
    "    print('networkx not installed — skipping visualisation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. PyTorch Geometric: Core Concepts\n",
    "\n",
    "### 5.1 What is PyTorch Geometric?\n",
    "\n",
    "[PyTorch Geometric (PyG)](https://pytorch-geometric.readthedocs.io/) provides:\n",
    "- A **`Data`** object to represent a single graph\n",
    "- A **`Dataset`** / **`DataLoader`** pipeline for batching graphs\n",
    "- A rich library of **GNN layers** (GCNConv, SAGEConv, GATConv, …)\n",
    "- **Mini-batch support** using sparse block-diagonal adjacency matrices\n",
    "\n",
    "### 5.2 The `Data` Object\n",
    "\n",
    "The central data structure in PyG is `torch_geometric.data.Data`:\n",
    "\n",
    "```python\n",
    "Data(\n",
    "    x          = <node features>       # [num_nodes, num_node_features]\n",
    "    edge_index = <COO edge index>      # [2, num_edges]\n",
    "    edge_attr  = <edge features>       # [num_edges, num_edge_features]  (optional)\n",
    "    y          = <labels>              # node-, edge-, or graph-level labels\n",
    "    pos        = <node positions>      # [num_nodes, num_dimensions]    (optional)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torch_geometric.data import Data\n",
    "\n",
    "    # Build the same 5-node graph\n",
    "    data = Data(\n",
    "        x=torch.randn(5, 4),          # 5 nodes, 4 features\n",
    "        edge_index=edge_index,\n",
    "        y=torch.tensor([0, 1, 0, 1, 0])  # node labels\n",
    "    )\n",
    "\n",
    "    print(data)\n",
    "    print('Num nodes:', data.num_nodes)\n",
    "    print('Num edges:', data.num_edges)\n",
    "    print('Num node features:', data.num_node_features)\n",
    "    print('Has isolated nodes:', data.has_isolated_nodes())\n",
    "    print('Has self-loops:', data.has_self_loops())\n",
    "    print('Is directed:', data.is_directed())\n",
    "except ImportError:\n",
    "    print('torch_geometric not installed — install it to run this cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Message Passing Framework\n",
    "\n",
    "Almost every GNN follows the **message passing** paradigm:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_v^{(k)} = \\text{UPDATE}^{(k)}\\!\\left(\n",
    "    \\mathbf{h}_v^{(k-1)},\n",
    "    \\text{AGGREGATE}^{(k)}\\!\\left(\n",
    "        \\left\\{ \\mathbf{m}_{(u,v)}^{(k)} : u \\in \\mathcal{N}(v) \\right\\}\n",
    "    \\right)\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "where $\\mathbf{m}_{(u,v)}^{(k)} = \\text{MESSAGE}^{(k)}(\\mathbf{h}_u^{(k-1)}, \\mathbf{h}_v^{(k-1)}, \\mathbf{e}_{uv})$.\n",
    "\n",
    "PyG implements this via `MessagePassing` base class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torch_geometric.nn import MessagePassing\n",
    "    from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "    class SimpleGCNConv(MessagePassing):\n",
    "        \"\"\"A minimal GCN convolution to illustrate MessagePassing.\"\"\"\n",
    "\n",
    "        def __init__(self, in_channels, out_channels):\n",
    "            super().__init__(aggr='add')  # 'add' aggregation\n",
    "            self.lin = nn.Linear(in_channels, out_channels, bias=False)\n",
    "\n",
    "        def forward(self, x, edge_index):\n",
    "            # 1. Add self-loops to the adjacency matrix\n",
    "            edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "            # 2. Linearly transform node feature matrix\n",
    "            x = self.lin(x)\n",
    "\n",
    "            # 3. Compute normalisation coefficients\n",
    "            row, col = edge_index\n",
    "            deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "            deg_inv_sqrt = deg.pow(-0.5)\n",
    "            norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "            # 4. Start propagating messages\n",
    "            return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "        def message(self, x_j, norm):\n",
    "            # x_j: features of source nodes\n",
    "            return norm.view(-1, 1) * x_j\n",
    "\n",
    "    conv = SimpleGCNConv(4, 8)\n",
    "    out = conv(data.x, data.edge_index)\n",
    "    print('Output shape:', out.shape)  # [5, 8]\n",
    "except ImportError:\n",
    "    print('torch_geometric not installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Datasets and DataLoaders in PyG\n",
    "\n",
    "### 6.1 Built-in Datasets\n",
    "\n",
    "PyG ships with many benchmark datasets, for example:\n",
    "\n",
    "| Dataset | Task | Nodes | Edges |\n",
    "|---------|------|-------|-------|\n",
    "| Cora | Node classification | 2708 | 10556 |\n",
    "| Citeseer | Node classification | 3327 | 9104 |\n",
    "| KarateClub | Community detection | 34 | 156 |\n",
    "| TUDataset (MUTAG) | Graph classification | ~18 | ~40 |\n",
    "\n",
    "### 6.2 The Karate Club Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torch_geometric.datasets import KarateClub\n",
    "\n",
    "    dataset = KarateClub()\n",
    "    data_kc = dataset[0]\n",
    "\n",
    "    print('Dataset:', dataset)\n",
    "    print('Number of graphs:', len(dataset))\n",
    "    print('Number of node features:', dataset.num_features)\n",
    "    print('Number of classes:', dataset.num_classes)\n",
    "    print()\n",
    "    print(data_kc)\n",
    "    print('Number of nodes:', data_kc.num_nodes)\n",
    "    print('Number of edges:', data_kc.num_edges)\n",
    "except ImportError:\n",
    "    print('torch_geometric not installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Mini-batching Graphs\n",
    "\n",
    "PyG's `DataLoader` stacks multiple graphs into a **single disconnected graph** using block-diagonal adjacency:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}_{\\text{batch}} = \\begin{pmatrix} A_1 & & \\\\ & A_2 & \\\\ & & \\ddots \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The `batch` vector maps each node to its graph index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torch_geometric.loader import DataLoader\n",
    "    from torch_geometric.datasets import TUDataset\n",
    "\n",
    "    dataset_tu = TUDataset(root='/tmp/MUTAG', name='MUTAG')\n",
    "    print('MUTAG:', dataset_tu)\n",
    "    print('Number of graphs:', len(dataset_tu))\n",
    "    print('Number of classes:', dataset_tu.num_classes)\n",
    "\n",
    "    loader = DataLoader(dataset_tu, batch_size=32, shuffle=True)\n",
    "    for batch in loader:\n",
    "        print('Batch:', batch)\n",
    "        print('Batch vector (unique values):', batch.batch.unique())\n",
    "        break\n",
    "except ImportError:\n",
    "    print('torch_geometric not installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Your First GNN Layer (`GCNConv`)\n",
    "\n",
    "### 7.1 Theory Recap\n",
    "\n",
    "The **Graph Convolutional Network (GCN)** layer (Kipf & Welling, 2017) computes:\n",
    "\n",
    "$$\n",
    "\\mathbf{H}^{(l+1)} = \\sigma\\!\\left(\n",
    "    \\hat{\\mathbf{D}}^{-1/2}\\hat{\\mathbf{A}}\\hat{\\mathbf{D}}^{-1/2}\\mathbf{H}^{(l)}\\mathbf{W}^{(l)}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "where $\\hat{\\mathbf{A}} = \\mathbf{A} + \\mathbf{I}$ (add self-loops) and $\\hat{\\mathbf{D}}$ is the diagonal degree matrix of $\\hat{\\mathbf{A}}$.\n",
    "\n",
    "### 7.2 Node Classification on Cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torch_geometric.datasets import Planetoid\n",
    "    from torch_geometric.nn import GCNConv\n",
    "\n",
    "    dataset_cora = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "    data_cora = dataset_cora[0]\n",
    "    print(data_cora)\n",
    "except ImportError:\n",
    "    print('torch_geometric not installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    class GCN(nn.Module):\n",
    "        def __init__(self, in_ch, hidden_ch, out_ch):\n",
    "            super().__init__()\n",
    "            self.conv1 = GCNConv(in_ch, hidden_ch)\n",
    "            self.conv2 = GCNConv(hidden_ch, out_ch)\n",
    "\n",
    "        def forward(self, data):\n",
    "            x, edge_index = data.x, data.edge_index\n",
    "            x = F.relu(self.conv1(x, edge_index))\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model_gcn = GCN(dataset_cora.num_features, 16, dataset_cora.num_classes).to(device)\n",
    "    data_cora = data_cora.to(device)\n",
    "    optimizer_gcn = optim.Adam(model_gcn.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "    def train():\n",
    "        model_gcn.train()\n",
    "        optimizer_gcn.zero_grad()\n",
    "        out = model_gcn(data_cora)\n",
    "        loss = F.nll_loss(out[data_cora.train_mask], data_cora.y[data_cora.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer_gcn.step()\n",
    "        return loss.item()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test():\n",
    "        model_gcn.eval()\n",
    "        out = model_gcn(data_cora)\n",
    "        pred = out.argmax(dim=1)\n",
    "        accs = []\n",
    "        for mask in [data_cora.train_mask, data_cora.val_mask, data_cora.test_mask]:\n",
    "            accs.append(int((pred[mask] == data_cora.y[mask]).sum()) / int(mask.sum()))\n",
    "        return accs\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        train()\n",
    "\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    print(f'Train: {train_acc:.4f} | Val: {val_acc:.4f} | Test: {test_acc:.4f}')\n",
    "except NameError:\n",
    "    print('torch_geometric not installed — skipping GCN training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Visualising Node Embeddings with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "\n",
    "    model_gcn.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get embeddings from first layer\n",
    "        x = data_cora.x\n",
    "        emb = F.relu(model_gcn.conv1(x, data_cora.edge_index))\n",
    "\n",
    "    emb_np = emb.cpu().numpy()\n",
    "    labels_np = data_cora.y.cpu().numpy()\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    emb_2d = tsne.fit_transform(emb_np)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(emb_2d[:, 0], emb_2d[:, 1], c=labels_np, cmap='tab10', s=10)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('t-SNE of GCN Node Embeddings (Cora)')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f'Skipping t-SNE visualisation: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Exercises\n",
    "\n",
    "### Exercise 1 — Tensor Operations\n",
    "1. Create a $4 \\times 4$ matrix of random integers between 0 and 9 using `torch.randint`.\n",
    "2. Compute its trace (sum of diagonal elements). *Hint: `torch.diagonal`*.\n",
    "3. Extract the upper-triangular part using `torch.triu`.\n",
    "4. Compute the L2 norm of each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 — your solution here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 — Custom `nn.Module`\n",
    "Implement a **residual block**: a module that applies two linear layers with ReLU activations and adds the input to the output (skip connection). Test it on random input.\n",
    "\n",
    "$$\\text{out} = \\text{ReLU}(W_2 \\cdot \\text{ReLU}(W_1 \\cdot x)) + x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 — your solution here\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        # TODO: define two linear layers\n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: implement residual connection\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 — Custom `MessagePassing` Layer\n",
    "Implement a **mean-aggregation** GNN layer using `MessagePassing` that:\n",
    "1. Aggregates neighbour features by **mean** (`aggr='mean'`).\n",
    "2. Concatenates the mean-aggregated neighbour features with the node's own features.\n",
    "3. Applies a linear transformation to the concatenated vector.\n",
    "\n",
    "Test it on the KarateClub dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 — your solution here\n",
    "try:\n",
    "    from torch_geometric.nn import MessagePassing\n",
    "\n",
    "    class MeanAggConv(MessagePassing):\n",
    "        def __init__(self, in_channels, out_channels):\n",
    "            super().__init__(aggr='mean')\n",
    "            # TODO: define linear layer(s)\n",
    "            ...\n",
    "\n",
    "        def forward(self, x, edge_index):\n",
    "            # TODO\n",
    "            ...\n",
    "\n",
    "        def message(self, x_j):\n",
    "            return x_j\n",
    "except ImportError:\n",
    "    print('torch_geometric not installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 — Graph Classification Pipeline\n",
    "Build a complete **graph classification** pipeline on the MUTAG dataset:\n",
    "1. Load MUTAG using `TUDataset`.\n",
    "2. Split into train / test sets (80/20).\n",
    "3. Use `DataLoader` with `batch_size=32`.\n",
    "4. Build a 2-layer GCN with global mean pooling (`global_mean_pool`).\n",
    "5. Train for 100 epochs and report test accuracy.\n",
    "\n",
    "*Hint:* `from torch_geometric.nn import global_mean_pool`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 — your solution here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Topic | Key Takeaway |\n",
    "|-------|--------------|\n",
    "| **PyTorch Tensors** | n-dimensional arrays with GPU support and autograd |\n",
    "| **Autograd** | Automatic gradient computation via backpropagation |\n",
    "| **`nn.Module`** | Base class for building reusable neural network modules |\n",
    "| **Graph data** | Represented as `(x, edge_index)` in COO format |\n",
    "| **PyG `Data`** | Flexible container for a single graph |\n",
    "| **`MessagePassing`** | Unified API for message-passing GNNs |\n",
    "| **`DataLoader`** | Efficient mini-batching of graph datasets |\n",
    "\n",
    "**Next notebook →** `02_gcn_graphsage_gat.ipynb` — GCN, GraphSAGE, and GAT in depth."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
