{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: GCN, GraphSAGE, and GAT\n",
    "\n",
    "This notebook covers three foundational **Graph Neural Network** architectures:\n",
    "\n",
    "| Model | Paper | Key Idea |\n",
    "|-------|-------|----------|\n",
    "| **GCN** | Kipf & Welling (2017) | Spectral convolution with symmetric normalisation |\n",
    "| **GraphSAGE** | Hamilton et al. (2017) | Inductive learning via sampled neighbourhood aggregation |\n",
    "| **GAT** | Veličković et al. (2018) | Attention-weighted aggregation |\n",
    "\n",
    "**Contents**\n",
    "1. [Graph Convolutional Network (GCN)](#1-graph-convolutional-network-gcn)\n",
    "2. [GraphSAGE](#2-graphsage)\n",
    "3. [Graph Attention Network (GAT)](#3-graph-attention-network-gat)\n",
    "4. [Comparison on Cora](#4-comparison-on-cora)\n",
    "5. [Exercises](#5-exercises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install\n",
    "# !pip install torch torch_geometric\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "\n",
    "try:\n",
    "    import torch_geometric\n",
    "    print(f'PyG version: {torch_geometric.__version__}')\n",
    "    pyg_available = True\n",
    "except ImportError:\n",
    "    print('PyTorch Geometric not installed — some cells will be skipped')\n",
    "    pyg_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyg_available:\n",
    "    from torch_geometric.datasets import Planetoid\n",
    "    from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
    "    import torch_geometric.transforms as T\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    dataset = Planetoid(root='/tmp/Cora', name='Cora',\n",
    "                        transform=T.NormalizeFeatures())\n",
    "    data = dataset[0].to(device)\n",
    "\n",
    "    print(data)\n",
    "    print(f'Classes: {dataset.num_classes}')\n",
    "    print(f'Node features: {dataset.num_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Graph Convolutional Network (GCN)\n",
    "\n",
    "### 1.1 Theoretical Background\n",
    "\n",
    "#### 1.1.1 Spectral Graph Theory Motivation\n",
    "\n",
    "Classical CNNs apply learnable filters in the spatial domain on grid-structured data (images). To generalise convolution to graphs, one approach uses the **graph Laplacian**:\n",
    "\n",
    "$$\n",
    "\\mathbf{L} = \\mathbf{D} - \\mathbf{A}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{D}$ is the diagonal degree matrix. The normalised Laplacian $\\tilde{\\mathbf{L}} = \\mathbf{D}^{-1/2}\\mathbf{L}\\mathbf{D}^{-1/2}$ is symmetric positive semi-definite with eigendecomposition $\\tilde{\\mathbf{L}} = \\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{U}^\\top$.\n",
    "\n",
    "A spectral graph convolution is defined as:\n",
    "$$\\mathbf{x} \\star g = \\mathbf{U}\\,g(\\boldsymbol{\\Lambda})\\,\\mathbf{U}^\\top \\mathbf{x}$$\n",
    "\n",
    "This is expensive ($O(N^2)$ for eigen-decomposition). Kipf & Welling simplify by using a first-order approximation and adding self-loops.\n",
    "\n",
    "#### 1.1.2 GCN Propagation Rule\n",
    "\n",
    "The final layer-wise propagation rule is:\n",
    "\n",
    "$$\n",
    "\\boxed{\\mathbf{H}^{(l+1)} = \\sigma\\!\\left(\n",
    "    \\hat{\\mathbf{D}}^{-1/2}\\hat{\\mathbf{A}}\\hat{\\mathbf{D}}^{-1/2}\\mathbf{H}^{(l)}\\mathbf{W}^{(l)}\n",
    "\\right)}\n",
    "$$\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| $\\hat{\\mathbf{A}} = \\mathbf{A} + \\mathbf{I}$ | Adjacency matrix with added self-loops |\n",
    "| $\\hat{\\mathbf{D}}_{ii} = \\sum_j \\hat{\\mathbf{A}}_{ij}$ | Degree matrix of $\\hat{\\mathbf{A}}$ |\n",
    "| $\\mathbf{W}^{(l)}$ | Learnable weight matrix |\n",
    "| $\\sigma$ | Activation function (e.g., ReLU) |\n",
    "\n",
    "**Message-passing interpretation:**  For each node $v$:\n",
    "$$\n",
    "h_v^{(l+1)} = \\sigma\\!\\left(\n",
    "    \\sum_{u \\in \\mathcal{N}(v) \\cup \\{v\\}} \\frac{1}{\\sqrt{\\hat{d}_v \\hat{d}_u}} \\mathbf{W}^{(l)} h_u^{(l)}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Each node aggregates its neighbours' features, normalised by degree.\n",
    "\n",
    "#### 1.1.3 Limitations\n",
    "- **Transductive**: the model must see all nodes during training (fixed graph)\n",
    "- **Depth**: stacking many layers causes **over-smoothing** (all node representations converge)\n",
    "- **Symmetric normalisation**: treats all neighbours equally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 GCN Implementation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "def gcn_norm(edge_index, num_nodes):\n",
    "    \"\"\"Compute D^{-1/2} A_hat D^{-1/2} as sparse edge weights.\"\"\"\n",
    "    # Build adjacency with self-loops\n",
    "    row = torch.cat([edge_index[0], torch.arange(num_nodes)])\n",
    "    col = torch.cat([edge_index[1], torch.arange(num_nodes)])\n",
    "\n",
    "    # Degree\n",
    "    deg = torch.zeros(num_nodes)\n",
    "    deg.scatter_add_(0, row, torch.ones(row.size(0)))\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "\n",
    "    norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "    new_edge_index = torch.stack([row, col], dim=0)\n",
    "    return new_edge_index, norm\n",
    "\n",
    "\n",
    "class GCNLayerFromScratch(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_ch, out_ch, bias=False)\n",
    "\n",
    "    def forward(self, x, edge_index, norm):\n",
    "        # 1. Linear transform\n",
    "        x = self.W(x)                          # [N, out_ch]\n",
    "        # 2. Aggregate (manual scatter)\n",
    "        row, col = edge_index\n",
    "        agg = torch.zeros_like(x)\n",
    "        agg.scatter_add_(0, col.unsqueeze(-1).expand_as(x[row]),\n",
    "                         norm.unsqueeze(-1) * x[row])\n",
    "        return agg\n",
    "\n",
    "\n",
    "print('GCNLayerFromScratch defined successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 GCN with PyG's `GCNConv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyg_available:\n",
    "    class GCN(nn.Module):\n",
    "        def __init__(self, in_ch, hidden_ch, out_ch, dropout=0.5):\n",
    "            super().__init__()\n",
    "            self.conv1 = GCNConv(in_ch, hidden_ch)\n",
    "            self.conv2 = GCNConv(hidden_ch, out_ch)\n",
    "            self.dropout = dropout\n",
    "\n",
    "        def forward(self, x, edge_index):\n",
    "            x = F.relu(self.conv1(x, edge_index))\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "\n",
    "    print('GCN model defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. GraphSAGE\n",
    "\n",
    "### 2.1 Theoretical Background\n",
    "\n",
    "**GraphSAGE** (Hamilton et al., 2017) — *SAmple and aggreGatE* — was designed for **inductive** learning: it can generalise to unseen nodes at test time without retraining.\n",
    "\n",
    "#### 2.1.1 Key Idea: Neighbourhood Sampling\n",
    "\n",
    "Instead of using the full neighbourhood (expensive for hub nodes), GraphSAGE samples a **fixed-size** set of neighbours $\\mathcal{S}(v) \\subseteq \\mathcal{N}(v)$.\n",
    "\n",
    "#### 2.1.2 Propagation Rule\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_{\\mathcal{N}(v)}^{(k)} = \\text{AGGREGATE}^{(k)}\\!\\left(\n",
    "    \\left\\{\\mathbf{h}_u^{(k-1)} : u \\in \\mathcal{S}(v)\\right\\}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_v^{(k)} = \\sigma\\!\\left(\n",
    "    \\mathbf{W}^{(k)} \\cdot \\text{CONCAT}\\!\\left(\\mathbf{h}_v^{(k-1)},\\, \\mathbf{h}_{\\mathcal{N}(v)}^{(k)}\\right)\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_v^{(k)} \\leftarrow \\frac{\\mathbf{h}_v^{(k)}}{\\|\\mathbf{h}_v^{(k)}\\|_2}\n",
    "$$\n",
    "\n",
    "#### 2.1.3 Aggregators\n",
    "\n",
    "| Aggregator | Formula |\n",
    "|------------|----------|\n",
    "| **Mean** | $\\frac{1}{|\\mathcal{S}(v)|}\\sum_{u}\\mathbf{h}_u$ |\n",
    "| **Max-pool** | $\\max(\\{\\sigma(\\mathbf{W}_{\\text{pool}}\\mathbf{h}_u + \\mathbf{b}) : u \\in \\mathcal{S}(v)\\})$ |\n",
    "| **LSTM** | Apply LSTM on a random permutation of $\\mathcal{S}(v)$ |\n",
    "\n",
    "#### 2.1.4 Inductive Setting\n",
    "\n",
    "The key advantage over GCN: the weight matrices $\\mathbf{W}^{(k)}$ are shared across all nodes. Given a new node, we only need its features and a sample of its neighbours — **no retraining required**.\n",
    "\n",
    "#### 2.1.5 Unsupervised Training via Graph-based Loss\n",
    "\n",
    "For unsupervised learning, GraphSAGE uses a **binary cross-entropy** loss:\n",
    "$$\n",
    "J = -\\log\\sigma(\\mathbf{z}_u^\\top \\mathbf{z}_v) - Q \\cdot \\mathbb{E}_{v_n \\sim P_n}[\\log\\sigma(-\\mathbf{z}_u^\\top \\mathbf{z}_{v_n})]\n",
    "$$\n",
    "where $v$ is a nearby node (short random walk) and $v_n$ is a noise node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 GraphSAGE with PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyg_available:\n",
    "    class GraphSAGE(nn.Module):\n",
    "        def __init__(self, in_ch, hidden_ch, out_ch, dropout=0.5):\n",
    "            super().__init__()\n",
    "            # aggr='mean' by default; also try 'max' or 'lstm'\n",
    "            self.conv1 = SAGEConv(in_ch, hidden_ch, aggr='mean')\n",
    "            self.conv2 = SAGEConv(hidden_ch, out_ch, aggr='mean')\n",
    "            self.dropout = dropout\n",
    "\n",
    "        def forward(self, x, edge_index):\n",
    "            x = F.relu(self.conv1(x, edge_index))\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "\n",
    "    print('GraphSAGE model defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Inductive Learning with Neighbourhood Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyg_available:\n",
    "    try:\n",
    "        from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "        # NeighborLoader samples a fixed number of neighbours per hop\n",
    "        train_loader = NeighborLoader(\n",
    "            data,\n",
    "            num_neighbors=[10, 5],   # 10 neighbours in hop-1, 5 in hop-2\n",
    "            batch_size=256,\n",
    "            input_nodes=data.train_mask,\n",
    "        )\n",
    "\n",
    "        # Peek at a mini-batch\n",
    "        for mini_batch in train_loader:\n",
    "            print('Mini-batch:', mini_batch)\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f'NeighborLoader demo skipped: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Graph Attention Network (GAT)\n",
    "\n",
    "### 3.1 Theoretical Background\n",
    "\n",
    "**GAT** (Veličković et al., 2018) introduces **attention coefficients** so that different neighbours contribute differently to each node's new representation.\n",
    "\n",
    "#### 3.1.1 Attention Mechanism\n",
    "\n",
    "Given node features $\\mathbf{h}_i, \\mathbf{h}_j \\in \\mathbb{R}^F$:\n",
    "\n",
    "**Step 1 – Linear projection:**\n",
    "$$\\mathbf{z}_i = \\mathbf{W}\\mathbf{h}_i, \\quad \\mathbf{z}_j = \\mathbf{W}\\mathbf{h}_j$$\n",
    "\n",
    "**Step 2 – Attention coefficient:**\n",
    "$$e_{ij} = \\text{LeakyReLU}\\!\\left(\\mathbf{a}^\\top [\\mathbf{z}_i \\| \\mathbf{z}_j]\\right)$$\n",
    "\n",
    "**Step 3 – Normalise with Softmax (over neighbours):**\n",
    "$$\\alpha_{ij} = \\text{softmax}_j(e_{ij}) = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}(i)} \\exp(e_{ik})}$$\n",
    "\n",
    "**Step 4 – Weighted aggregation:**\n",
    "$$\\mathbf{h}_i^{\\prime} = \\sigma\\!\\left(\\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij} \\mathbf{z}_j\\right)$$\n",
    "\n",
    "#### 3.1.2 Multi-Head Attention\n",
    "\n",
    "To stabilise training, GAT uses $K$ independent attention heads:\n",
    "\n",
    "$$\\mathbf{h}_i^{\\prime} = \\big\\|_{k=1}^K \\sigma\\!\\left(\\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij}^{(k)} \\mathbf{W}^{(k)}\\mathbf{h}_j\\right)$$\n",
    "\n",
    "For the final layer, averaging is used instead of concatenation:\n",
    "$$\\mathbf{h}_i^{\\prime} = \\sigma\\!\\left(\\frac{1}{K}\\sum_{k=1}^K \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij}^{(k)} \\mathbf{W}^{(k)}\\mathbf{h}_j\\right)$$\n",
    "\n",
    "#### 3.1.3 GATv2\n",
    "\n",
    "Brody et al. (2022) proposed **GATv2**, which fixes an expressiveness issue in the original GAT. The attention score is computed as:\n",
    "$$e_{ij} = \\mathbf{a}^\\top \\text{LeakyReLU}\\!\\left(\\mathbf{W}[\\mathbf{h}_i \\| \\mathbf{h}_j]\\right)$$\n",
    "\n",
    "This is a *dynamic* attention mechanism (the linear projection happens *after* concatenation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 GAT from Scratch (Illustrative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayerFromScratch(nn.Module):\n",
    "    \"\"\"Single-head GAT layer (illustrative, not optimised).\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout=0.6, alpha=0.2):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(in_features, out_features, bias=False)\n",
    "        # Attention vector: a in R^{2*out_features}\n",
    "        self.a = nn.Parameter(torch.empty(2 * out_features))\n",
    "        nn.init.xavier_uniform_(self.a.unsqueeze(0))\n",
    "        self.leaky_relu = nn.LeakyReLU(alpha)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        N = x.size(0)\n",
    "        z = self.W(x)                          # [N, out_features]\n",
    "\n",
    "        row, col = edge_index                  # source, target\n",
    "        # Concatenate [z_i || z_j] for each edge\n",
    "        z_cat = torch.cat([z[row], z[col]], dim=-1)  # [E, 2*out]\n",
    "        e = self.leaky_relu((z_cat * self.a).sum(-1)) # [E]\n",
    "\n",
    "        # Softmax over incoming edges for each target node\n",
    "        # Use scatter_softmax from PyG or manual implementation\n",
    "        # (manual for illustration)\n",
    "        e_max = torch.full((N,), float('-inf'))\n",
    "        e_max.scatter_reduce_(0, col, e, reduce='amax', include_self=True)\n",
    "        e_exp = (e - e_max[col]).exp()\n",
    "        e_sum = torch.zeros(N).scatter_add_(0, col, e_exp)\n",
    "        alpha = e_exp / (e_sum[col] + 1e-16)\n",
    "\n",
    "        # Weighted aggregation\n",
    "        out = torch.zeros_like(z)\n",
    "        out.scatter_add_(0, col.unsqueeze(-1).expand_as(z[row]),\n",
    "                         alpha.unsqueeze(-1) * z[row])\n",
    "        return F.elu(out)\n",
    "\n",
    "\n",
    "print('GATLayerFromScratch defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 GAT with PyG's `GATConv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyg_available:\n",
    "    class GAT(nn.Module):\n",
    "        def __init__(self, in_ch, hidden_ch, out_ch, heads=8, dropout=0.6):\n",
    "            super().__init__()\n",
    "            # First layer: 8 heads, concat=True => hidden_ch * heads output\n",
    "            self.conv1 = GATConv(in_ch, hidden_ch, heads=heads,\n",
    "                                 dropout=dropout, concat=True)\n",
    "            # Final layer: 1 head, concat=False => average\n",
    "            self.conv2 = GATConv(hidden_ch * heads, out_ch, heads=1,\n",
    "                                 dropout=dropout, concat=False)\n",
    "            self.dropout = dropout\n",
    "\n",
    "        def forward(self, x, edge_index):\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = F.elu(self.conv1(x, edge_index))\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "\n",
    "    print('GAT model defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Visualising Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyg_available:\n",
    "    # Inspect attention weights from a trained GAT\n",
    "    # GATConv returns (out, (edge_index, alpha)) when return_attention_weights=True\n",
    "    gat_layer = GATConv(dataset.num_features, 8, heads=1,\n",
    "                        concat=False, return_attention_weights=True)\n",
    "    gat_layer = gat_layer.to(device)\n",
    "\n",
    "    gat_layer.eval()\n",
    "    with torch.no_grad():\n",
    "        out, (att_edge_index, att_weights) = gat_layer(data.x, data.edge_index,\n",
    "                                                        return_attention_weights=True)\n",
    "\n",
    "    print('Attention weights shape:', att_weights.shape)  # [num_edges, 1]\n",
    "    print('Min attention:', att_weights.min().item())\n",
    "    print('Max attention:', att_weights.max().item())\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.hist(att_weights.cpu().squeeze().numpy(), bins=50)\n",
    "    plt.xlabel('Attention weight $\\\\alpha_{ij}$')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of GAT Attention Weights on Cora')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Comparison on Cora\n",
    "\n",
    "### 4.1 Training Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyg_available:\n",
    "    def train_model(model, data, epochs=200, lr=0.01, wd=5e-4):\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        train_losses, val_accs = [], []\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index)\n",
    "            loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred = out.argmax(dim=1)\n",
    "                val_acc = (pred[data.val_mask] == data.y[data.val_mask]).float().mean()\n",
    "            val_accs.append(val_acc.item())\n",
    "\n",
    "        return train_losses, val_accs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(model, data):\n",
    "        model.eval()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred = out.argmax(dim=1)\n",
    "        results = {}\n",
    "        for split in ['train_mask', 'val_mask', 'test_mask']:\n",
    "            mask = getattr(data, split)\n",
    "            acc = (pred[mask] == data.y[mask]).float().mean().item()\n",
    "            results[split.replace('_mask', '')] = acc\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyg_available:\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    models = {\n",
    "        'GCN': GCN(dataset.num_features, 64, dataset.num_classes).to(device),\n",
    "        'GraphSAGE': GraphSAGE(dataset.num_features, 64, dataset.num_classes).to(device),\n",
    "        'GAT': GAT(dataset.num_features, 8, dataset.num_classes).to(device),\n",
    "    }\n",
    "\n",
    "    histories = {}\n",
    "    for name, model in models.items():\n",
    "        print(f'Training {name}...')\n",
    "        losses, val_accs = train_model(model, data, epochs=200)\n",
    "        histories[name] = (losses, val_accs)\n",
    "        results = evaluate(model, data)\n",
    "        print(f'  Train: {results[\"train\"]:.4f} | Val: {results[\"val\"]:.4f} | Test: {results[\"test\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyg_available and histories:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    for name, (losses, val_accs) in histories.items():\n",
    "        axes[0].plot(losses, label=name)\n",
    "        axes[1].plot(val_accs, label=name)\n",
    "\n",
    "    axes[0].set_title('Training Loss'); axes[0].set_xlabel('Epoch'); axes[0].legend()\n",
    "    axes[1].set_title('Validation Accuracy'); axes[1].set_xlabel('Epoch'); axes[1].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Summary of Differences\n",
    "\n",
    "| Property | GCN | GraphSAGE | GAT |\n",
    "|----------|-----|-----------|-----|\n",
    "| Neighbour weighting | Symmetric degree normalisation | Uniform (mean) or other | Learned attention |\n",
    "| Self-loop handling | Explicit (add $\\mathbf{I}$) | Concatenate self | Masked self-attention |\n",
    "| Inductive capability | ✗ (transductive) | ✓ | ✓ (with caution) |\n",
    "| Neighbour sampling | No | Yes | No |\n",
    "| Computational cost | $O(|E| \\cdot F)$ | $O(S \\cdot F)$ | $O(|E| \\cdot F)$ |\n",
    "| # parameters | Low | Low | High (multi-head) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Exercises\n",
    "\n",
    "### Exercise 1 — Depth vs. Over-Smoothing in GCN\n",
    "\n",
    "Train GCN models with 2, 4, 6, and 8 layers on Cora. For each depth:\n",
    "1. Report test accuracy.\n",
    "2. Compute the **mean cosine similarity** between all pairs of test node representations.\n",
    "3. Plot accuracy and mean cosine similarity vs. depth.\n",
    "\n",
    "Observe over-smoothing as depth increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 — your solution here\n",
    "if pyg_available:\n",
    "    class DeepGCN(nn.Module):\n",
    "        def __init__(self, in_ch, hidden_ch, out_ch, num_layers):\n",
    "            super().__init__()\n",
    "            # TODO: build a GCN with num_layers layers\n",
    "            ...\n",
    "\n",
    "        def forward(self, x, edge_index):\n",
    "            ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 — GraphSAGE Aggregators\n",
    "\n",
    "Train three versions of GraphSAGE on Cora using different aggregators: `'mean'`, `'max'`, and `'lstm'`.\n",
    "Compare their test accuracy and convergence speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 — your solution here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 — Multi-Head Attention Ablation\n",
    "\n",
    "Train GAT with `heads ∈ {1, 2, 4, 8}` on Cora. Plot test accuracy vs. number of heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 — your solution here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 — Graph Classification with GIN\n",
    "\n",
    "**Graph Isomorphism Network (GIN)** (Xu et al., 2019) is the most powerful GNN in the WL hierarchy:\n",
    "\n",
    "$$h_v^{(k)} = \\text{MLP}^{(k)}\\!\\left((1 + \\epsilon^{(k)})\\cdot h_v^{(k-1)} + \\sum_{u \\in \\mathcal{N}(v)} h_u^{(k-1)}\\right)$$\n",
    "\n",
    "Using PyG's `GINConv` and global pooling:\n",
    "1. Load the MUTAG dataset.\n",
    "2. Build a 3-layer GIN classifier with `global_add_pool`.\n",
    "3. Train for 100 epochs and compare with GCN-based classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 — your solution here\n",
    "if pyg_available:\n",
    "    from torch_geometric.nn import GINConv, global_add_pool\n",
    "    from torch_geometric.datasets import TUDataset\n",
    "    from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "\n",
    "    class GINClassifier(nn.Module):\n",
    "        def __init__(self, in_ch, hidden_ch, out_ch):\n",
    "            super().__init__()\n",
    "            # TODO: 3 GINConv layers + MLP classifier\n",
    "            ...\n",
    "\n",
    "        def forward(self, x, edge_index, batch):\n",
    "            # TODO\n",
    "            ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 — Attention Map Visualisation\n",
    "\n",
    "After training GAT on Cora:\n",
    "1. Extract attention weights from both layers.\n",
    "2. For a chosen node, visualise its ego-graph (1-hop neighbourhood) with edge thickness proportional to attention weight.\n",
    "\n",
    "*Hint: use `networkx` and `matplotlib` with custom edge widths.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5 — your solution here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Model | Strengths | Weaknesses |\n",
    "|-------|-----------|------------|\n",
    "| **GCN** | Simple, fast, effective for dense/homophilic graphs | Transductive, over-smoothing, equal neighbour weighting |\n",
    "| **GraphSAGE** | Inductive, scalable with sampling | Uniform aggregation (mean), needs careful sampling |\n",
    "| **GAT** | Learned neighbour importance, interpretable | More parameters, sensitive to hyperparameters |\n",
    "\n",
    "**Next notebook →** `03_knowledge_graph_embeddings.ipynb` — TransE, TransR, RotatE, ComplEx, LiteralE, and Heterogeneous Graphs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
