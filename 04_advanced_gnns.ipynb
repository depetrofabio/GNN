{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: Advanced GNNs — Graph Transformer, Heterogeneous Graph Transformer, R-GCN\n",
    "\n",
    "This notebook covers three advanced GNN architectures that push beyond standard message passing:\n",
    "\n",
    "| Model | Paper | Key Idea |\n",
    "|-------|-------|----------|\n",
    "| **Graph Transformer (GT / Graphormer)** | Dwivedi & Bresson (2021); Ying et al. (2021) | Transformer self-attention on graph nodes |\n",
    "| **Heterogeneous Graph Transformer (HGT)** | Hu et al. (2020) | Relation-aware attention for heterogeneous graphs |\n",
    "| **R-GCN** | Schlichtkrull et al. (2018) | Relation-specific weights for relational graphs |\n",
    "\n",
    "**Contents**\n",
    "1. [Graph Transformer (GT)](#1-graph-transformer)\n",
    "2. [Graphormer](#2-graphormer)\n",
    "3. [Heterogeneous Graph Transformer (HGT)](#3-heterogeneous-graph-transformer-hgt)\n",
    "4. [Relational GCN (R-GCN)](#4-relational-gcn-r-gcn)\n",
    "5. [Comparison Experiment](#5-comparison-experiment)\n",
    "6. [Exercises](#6-exercises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install\n",
    "# !pip install torch torch_geometric\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "\n",
    "try:\n",
    "    import torch_geometric\n",
    "    from torch_geometric.datasets import Planetoid, DBLP\n",
    "    import torch_geometric.transforms as T\n",
    "    print(f'PyG version: {torch_geometric.__version__}')\n",
    "    pyg_available = True\n",
    "except ImportError:\n",
    "    print('PyTorch Geometric not installed')\n",
    "    pyg_available = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Graph Transformer\n",
    "\n",
    "### 1.1 Motivation: Limitations of Standard GNNs\n",
    "\n",
    "Standard message-passing GNNs have a few known limitations:\n",
    "\n",
    "| Limitation | Description |\n",
    "|------------|-------------|\n",
    "| **Over-smoothing** | Deep GNNs converge to indistinguishable representations |\n",
    "| **Over-squashing** | Information from distant nodes is compressed into fixed-size vectors |\n",
    "| **Limited expressiveness** | Bounded by the 1-Weisfeiler-Leman (1-WL) test |\n",
    "| **Local neighbourhood only** | Long-range dependencies require many layers |\n",
    "\n",
    "**Transformers** (Vaswani et al., 2017) address many of these with **global self-attention**, which allows every node to attend to every other node in a single step.\n",
    "\n",
    "### 1.2 Vanilla Self-Attention on Graphs\n",
    "\n",
    "The simplest approach is to apply **standard multi-head attention (MHA)** treating nodes as tokens:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "where $Q = X W_Q$, $K = X W_K$, $V = X W_V$, and $X$ is the node feature matrix.\n",
    "\n",
    "Without any graph bias this is equivalent to a fully connected graph — the graph structure is ignored.\n",
    "\n",
    "### 1.3 Graph Transformer (Dwivedi & Bresson, 2021)\n",
    "\n",
    "The **Graph Transformer** layer incorporates graph structure by:\n",
    "1. **Masking attention** to only attend within the neighbourhood (local attention)\n",
    "2. Using **Laplacian positional encodings (LPE)** to inject structural information\n",
    "\n",
    "#### Laplacian Positional Encoding\n",
    "\n",
    "The top-$k$ eigenvectors of the normalised graph Laplacian are appended to node features:\n",
    "$$\\mathbf{x}_v \\leftarrow \\mathbf{x}_v \\| [\\lambda_1^v, \\lambda_2^v, \\ldots, \\lambda_k^v]$$\n",
    "\n",
    "This gives nodes a position-like signal that reflects the graph topology.\n",
    "\n",
    "#### GT Layer Formula\n",
    "\n",
    "$$\\hat{h}_i = \\text{MHA}(h_i, \\{h_j : j \\in \\mathcal{N}(i)\\})$$\n",
    "\n",
    "$$h_i^{\\prime} = \\text{LayerNorm}(h_i + \\hat{h}_i)$$\n",
    "\n",
    "$$h_i^{\\prime\\prime} = \\text{LayerNorm}(h_i^{\\prime} + \\text{FFN}(h_i^{\\prime}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Graph Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphTransformerLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Transformer layer (Dwivedi & Bresson, 2021).\n",
    "    Applies multi-head attention with a sparse mask (edge mask)\n",
    "    so only neighbours can attend to each other.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1, use_edge_bias=True):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        x:          [N, d_model]\n",
    "        edge_index: [2, E]  (directed COO format)\n",
    "        Returns: [N, d_model]\n",
    "        \"\"\"\n",
    "        N = x.size(0)\n",
    "        residual = x\n",
    "\n",
    "        Q = self.W_Q(x).view(N, self.num_heads, self.d_k)   # [N, H, d_k]\n",
    "        K = self.W_K(x).view(N, self.num_heads, self.d_k)\n",
    "        V = self.W_V(x).view(N, self.num_heads, self.d_k)\n",
    "\n",
    "        src, dst = edge_index   # src -> dst\n",
    "\n",
    "        # Attention score per edge: Q[dst] · K[src] / sqrt(d_k)\n",
    "        q_dst = Q[dst]           # [E, H, d_k]\n",
    "        k_src = K[src]           # [E, H, d_k]\n",
    "        attn  = (q_dst * k_src).sum(-1) / math.sqrt(self.d_k)  # [E, H]\n",
    "\n",
    "        # Softmax over in-edges for each dst node (manual scatter-softmax)\n",
    "        attn_max = torch.full((N, self.num_heads), float('-inf'), device=x.device)\n",
    "        attn_max.scatter_reduce_(0,\n",
    "                                 dst.unsqueeze(-1).expand_as(attn),\n",
    "                                 attn, reduce='amax', include_self=True)\n",
    "        attn_exp = (attn - attn_max[dst]).exp()\n",
    "        attn_sum = torch.zeros(N, self.num_heads, device=x.device)\n",
    "        attn_sum.scatter_add_(0, dst.unsqueeze(-1).expand_as(attn_exp), attn_exp)\n",
    "        alpha = attn_exp / (attn_sum[dst] + 1e-16)    # [E, H]\n",
    "        alpha = self.dropout(alpha)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        v_src = V[src]    # [E, H, d_k]\n",
    "        agg = torch.zeros(N, self.num_heads, self.d_k, device=x.device)\n",
    "        agg.scatter_add_(0,\n",
    "                         dst.unsqueeze(-1).unsqueeze(-1).expand_as(v_src),\n",
    "                         alpha.unsqueeze(-1) * v_src)\n",
    "\n",
    "        out = agg.view(N, self.d_model)    # [N, d_model]\n",
    "        out = self.W_O(out)\n",
    "\n",
    "        # Pre-LN residual\n",
    "        x = self.norm1(residual + self.dropout(out))\n",
    "        x = self.norm2(x + self.dropout(self.ffn(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "print('GraphTransformerLayer defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphTransformer(nn.Module):\n",
    "    \"\"\"Graph Transformer for node classification.\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, d_model, num_heads, num_layers, out_ch, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(in_ch, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            GraphTransformerLayer(d_model, num_heads, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Linear(d_model, out_ch)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.input_proj(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index)\n",
    "        return F.log_softmax(self.classifier(x), dim=1)\n",
    "\n",
    "\n",
    "# Quick sanity check\n",
    "N, F_in, d, H, L, C = 10, 16, 32, 4, 2, 5\n",
    "x_test = torch.randn(N, F_in)\n",
    "ei_test = torch.randint(0, N, (2, 20))\n",
    "gt_model = GraphTransformer(F_in, d, H, L, C)\n",
    "out_test = gt_model(x_test, ei_test)\n",
    "print('GraphTransformer output shape:', out_test.shape)  # [N, C]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Graph Transformer with PyG's `TransformerConv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyg_available:\n",
    "    from torch_geometric.nn import TransformerConv\n",
    "\n",
    "    class PyGGraphTransformer(nn.Module):\n",
    "        def __init__(self, in_ch, hidden_ch, out_ch, heads=4, num_layers=2, dropout=0.1):\n",
    "            super().__init__()\n",
    "            self.convs = nn.ModuleList()\n",
    "            self.norms = nn.ModuleList()\n",
    "\n",
    "            in_c = in_ch\n",
    "            for i in range(num_layers):\n",
    "                out_c = out_ch if i == num_layers - 1 else hidden_ch\n",
    "                concat = (i < num_layers - 1)  # concat on all but last layer\n",
    "                self.convs.append(\n",
    "                    TransformerConv(in_c, out_c, heads=heads,\n",
    "                                   dropout=dropout, concat=concat)\n",
    "                )\n",
    "                self.norms.append(nn.LayerNorm(out_c * heads if concat else out_c))\n",
    "                in_c = out_c * heads if concat else out_c\n",
    "\n",
    "            self.dropout = dropout\n",
    "\n",
    "        def forward(self, x, edge_index):\n",
    "            for i, (conv, norm) in enumerate(zip(self.convs, self.norms)):\n",
    "                x = conv(x, edge_index)\n",
    "                x = norm(x)\n",
    "                if i < len(self.convs) - 1:\n",
    "                    x = F.elu(x)\n",
    "                    x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "\n",
    "    print('PyGGraphTransformer defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Graphormer\n",
    "\n",
    "### 2.1 Theory\n",
    "\n",
    "**Graphormer** (Ying et al., 2021) achieves strong performance on molecular property prediction (OGB-LSC, PCQM4M). It augments standard Transformer with **graph-specific structural encodings**:\n",
    "\n",
    "#### 2.1.1 Centrality Encoding\n",
    "\n",
    "Degree information is injected as an additive bias to node features:\n",
    "$$\\mathbf{h}_i^{(0)} = \\mathbf{x}_i + z_{\\text{deg}^-(i)} + z_{\\text{deg}^+(i)}$$\n",
    "\n",
    "where $z_{\\text{deg}}$ are learnable degree embeddings.\n",
    "\n",
    "#### 2.1.2 Spatial Encoding\n",
    "\n",
    "A learnable scalar bias $b_{\\phi(v_i, v_j)}$ is added to the attention score based on the **shortest path distance** $\\phi(v_i, v_j)$:\n",
    "\n",
    "$$A_{ij} = \\frac{(Q_i)(K_j)^\\top}{\\sqrt{d}} + b_{\\phi(v_i, v_j)}$$\n",
    "\n",
    "This allows nodes at different graph distances to have different attention biases.\n",
    "\n",
    "#### 2.1.3 Edge Encoding\n",
    "\n",
    "Edge features on the shortest path from $v_i$ to $v_j$ are averaged and used as an additional attention bias:\n",
    "$$c_{ij} = \\frac{1}{N_{ij}} \\sum_{n=1}^{N_{ij}} \\mathbf{x}_{e_n}^\\top \\mathbf{w}_n$$\n",
    "\n",
    "#### 2.1.4 Virtual Node (VNode)\n",
    "\n",
    "A **virtual node** (VNODE) is added and connected to all real nodes. It aggregates global information and can be used as a graph-level representation.\n",
    "\n",
    "#### Summary of Graphormer Contributions\n",
    "\n",
    "| Component | Role |\n",
    "|-----------|------|\n",
    "| Centrality encoding | Node importance (degree) |\n",
    "| Spatial encoding | Graph distance between nodes |\n",
    "| Edge encoding | Edge feature integration |\n",
    "| Virtual node | Global graph representation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphormerLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified Graphormer layer demonstrating spatial encoding bias.\n",
    "    (Full Graphormer also includes edge encoding; omitted here for clarity.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, max_dist=5, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Spatial bias: one learnable scalar per distance bucket and per head\n",
    "        self.spatial_bias = nn.Embedding(max_dist + 2, num_heads)  # +2: 0 (self), -1 (unreachable)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, dist_matrix):\n",
    "        \"\"\"\n",
    "        x:           [N, d_model]\n",
    "        dist_matrix: [N, N] integer shortest-path distances (-1 = unreachable)\n",
    "        \"\"\"\n",
    "        N = x.size(0)\n",
    "\n",
    "        Q = self.W_Q(x).view(N, self.num_heads, self.d_k).transpose(0, 1)  # [H, N, d_k]\n",
    "        K = self.W_K(x).view(N, self.num_heads, self.d_k).transpose(0, 1)\n",
    "        V = self.W_V(x).view(N, self.num_heads, self.d_k).transpose(0, 1)\n",
    "\n",
    "        # Standard attention scores\n",
    "        attn = torch.bmm(Q, K.transpose(1, 2)) / math.sqrt(self.d_k)  # [H, N, N]\n",
    "\n",
    "        # Add spatial bias\n",
    "        # Clamp dist: unreachable (-1) -> max_dist+1, > max_dist -> max_dist\n",
    "        d = dist_matrix.clone()\n",
    "        d[d < 0] = self.spatial_bias.num_embeddings - 1\n",
    "        d = d.clamp(max=self.spatial_bias.num_embeddings - 1)\n",
    "        bias = self.spatial_bias(d)           # [N, N, H]\n",
    "        bias = bias.permute(2, 0, 1)          # [H, N, N]\n",
    "        attn = attn + bias\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.bmm(attn, V)              # [H, N, d_k]\n",
    "        out = out.transpose(0, 1).contiguous().view(N, self.d_model)\n",
    "        out = self.W_O(out)\n",
    "\n",
    "        x = self.norm1(x + self.dropout(out))\n",
    "        x = self.norm2(x + self.dropout(self.ffn(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test with a random distance matrix\n",
    "N, d, H = 8, 16, 2\n",
    "x_g = torch.randn(N, d)\n",
    "dist_m = torch.randint(0, 5, (N, N))\n",
    "dist_m.fill_diagonal_(0)\n",
    "gl = GraphormerLayer(d, H)\n",
    "out_g = gl(x_g, dist_m)\n",
    "print('Graphormer layer output shape:', out_g.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Heterogeneous Graph Transformer (HGT)\n",
    "\n",
    "### 3.1 Theory\n",
    "\n",
    "**HGT** (Hu et al., 2020) generalises the Transformer to **heterogeneous graphs** by defining **type-specific attention** for every combination of source node type, edge type, and target node type.\n",
    "\n",
    "#### 3.1.1 HGT Attention\n",
    "\n",
    "For a target node $t$ of type $\\tau(t)$ and a source node $s$ of type $\\tau(s)$ connected by edge type $\\phi(e)$:\n",
    "\n",
    "**Multi-head attention score:**\n",
    "$$\\text{Attn}^{(i)}(s, e, t) = \\frac{\\mathbf{K}^{(i)}(s) \\cdot \\mathbf{Q}^{(i)}(t)^\\top}{\\sqrt{d/h}} \\cdot \\mathbf{W}^{ATT}_{\\phi(e)}$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{K}^{(i)}(s) = \\mathbf{h}_s \\mathbf{W}_{\\tau(s)}^{K,i}$ — type-specific key projection\n",
    "- $\\mathbf{Q}^{(i)}(t) = \\mathbf{h}_t \\mathbf{W}_{\\tau(t)}^{Q,i}$ — type-specific query projection\n",
    "- $\\mathbf{W}^{ATT}_{\\phi(e)} \\in \\mathbb{R}^{d/h \\times d/h}$ — edge-type-specific attention matrix\n",
    "\n",
    "**Message:**\n",
    "$$\\text{Msg}^{(i)}(s, e, t) = \\mathbf{h}_s \\mathbf{W}_{\\tau(s)}^{V,i} \\cdot \\mathbf{W}^{MSG}_{\\phi(e)}$$\n",
    "\n",
    "**Aggregation:**\n",
    "$$\\tilde{\\mathbf{h}}_t = \\bigoplus_{i=1}^{h} \\sum_{s \\in \\mathcal{N}(t)} \\text{softmax}(\\text{Attn}^{(i)}) \\cdot \\text{Msg}^{(i)}$$\n",
    "\n",
    "**Update:**\n",
    "$$\\mathbf{h}_t^{(l+1)} = \\sigma\\!\\left(\\text{LayerNorm}(\\tilde{\\mathbf{h}}_t \\mathbf{W}_{\\tau(t)}^{A})\\right)$$\n",
    "\n",
    "#### 3.1.2 Key Properties\n",
    "\n",
    "| Property | Description |\n",
    "|----------|-------------|\n",
    "| **Type-specific projections** | Each node type has its own Q, K, V matrices |\n",
    "| **Relation-specific attention** | Edge types modulate attention scores and messages |\n",
    "| **Scalable** | Attention is sparse (only along edges) |\n",
    "| **Inductive** | Can handle new nodes at test time |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 HGT with PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyg_available:\n",
    "    from torch_geometric.nn import HGTConv, Linear as PyGLinear\n",
    "    from torch_geometric.data import HeteroData\n",
    "\n",
    "    # Re-create the academic heterogeneous graph from Notebook 3\n",
    "    hdata = HeteroData()\n",
    "    hdata['author'].x  = torch.randn(4, 8)\n",
    "    hdata['paper'].x   = torch.randn(6, 16)\n",
    "    hdata['venue'].x   = torch.randn(2, 4)\n",
    "\n",
    "    hdata['author', 'writes', 'paper'].edge_index = torch.tensor([\n",
    "        [0, 1, 2, 3, 0], [0, 1, 2, 3, 4]\n",
    "    ])\n",
    "    hdata['paper', 'cites', 'paper'].edge_index = torch.tensor([\n",
    "        [0, 1, 2, 3], [1, 2, 3, 4]\n",
    "    ])\n",
    "    hdata['paper', 'publishedIn', 'venue'].edge_index = torch.tensor([\n",
    "        [0, 1, 2, 3, 4, 5], [0, 0, 1, 1, 0, 1]\n",
    "    ])\n",
    "\n",
    "    print(hdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyg_available:\n",
    "    class HGT(nn.Module):\n",
    "        def __init__(self, hidden_channels, out_channels, num_heads, num_layers, data):\n",
    "            super().__init__()\n",
    "\n",
    "            # Input linear projections per node type (to unified hidden_channels)\n",
    "            self.lin_dict = nn.ModuleDict()\n",
    "            for ntype in data.node_types:\n",
    "                in_ch = data[ntype].x.size(-1)\n",
    "                self.lin_dict[ntype] = PyGLinear(in_ch, hidden_channels)\n",
    "\n",
    "            # HGT layers\n",
    "            self.convs = nn.ModuleList([\n",
    "                HGTConv(hidden_channels, hidden_channels,\n",
    "                        metadata=data.metadata(), heads=num_heads)\n",
    "                for _ in range(num_layers)\n",
    "            ])\n",
    "\n",
    "            # Output classifier (for 'paper' nodes)\n",
    "            self.lin_out = PyGLinear(hidden_channels, out_channels)\n",
    "\n",
    "        def forward(self, x_dict, edge_index_dict):\n",
    "            # Project all node types to hidden_channels\n",
    "            x_dict = {\n",
    "                ntype: self.lin_dict[ntype](x).relu_()\n",
    "                for ntype, x in x_dict.items()\n",
    "            }\n",
    "            # Heterogeneous transformer layers\n",
    "            for conv in self.convs:\n",
    "                x_dict = conv(x_dict, edge_index_dict)\n",
    "            # Classify paper nodes\n",
    "            return self.lin_out(x_dict['paper'])\n",
    "\n",
    "\n",
    "    hgt_model = HGT(hidden_channels=32, out_channels=4,\n",
    "                    num_heads=2, num_layers=2, data=hdata)\n",
    "    out_hgt = hgt_model(hdata.x_dict, hdata.edge_index_dict)\n",
    "    print('HGT output (paper nodes):', out_hgt.shape)  # [6, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 HGT on DBLP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyg_available:\n",
    "    try:\n",
    "        # DBLP: author classification (4 areas: DB, DM, IR, ML)\n",
    "        # Node types: author, paper, term, conference\n",
    "        dataset_dblp = DBLP(root='/tmp/DBLP', transform=T.Constant(node_types='conference'))\n",
    "        dblp_data = dataset_dblp[0]\n",
    "        print(dblp_data)\n",
    "        print('Author labels:', dblp_data['author'].y.unique())\n",
    "    except Exception as e:\n",
    "        print(f'DBLP dataset not available: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyg_available:\n",
    "    try:\n",
    "        class HGT_DBLP(nn.Module):\n",
    "            def __init__(self, data, hidden_ch=64, out_ch=4, heads=4, num_layers=2):\n",
    "                super().__init__()\n",
    "                self.lin_dict = nn.ModuleDict()\n",
    "                for ntype in data.node_types:\n",
    "                    in_c = data[ntype].x.size(-1)\n",
    "                    self.lin_dict[ntype] = PyGLinear(in_c, hidden_ch)\n",
    "\n",
    "                self.convs = nn.ModuleList([\n",
    "                    HGTConv(hidden_ch, hidden_ch, metadata=data.metadata(), heads=heads)\n",
    "                    for _ in range(num_layers)\n",
    "                ])\n",
    "                self.classifier = PyGLinear(hidden_ch, out_ch)\n",
    "\n",
    "            def forward(self, x_dict, edge_index_dict):\n",
    "                x_dict = {nt: self.lin_dict[nt](x).relu_() for nt, x in x_dict.items()}\n",
    "                for conv in self.convs:\n",
    "                    x_dict = conv(x_dict, edge_index_dict)\n",
    "                return self.classifier(x_dict['author'])\n",
    "\n",
    "\n",
    "        dblp_data = dblp_data.to(device)\n",
    "        hgt_dblp  = HGT_DBLP(dblp_data).to(device)\n",
    "        opt_hgt   = optim.Adam(hgt_dblp.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "\n",
    "        def train_hgt():\n",
    "            hgt_dblp.train()\n",
    "            opt_hgt.zero_grad()\n",
    "            out = hgt_dblp(dblp_data.x_dict, dblp_data.edge_index_dict)\n",
    "            loss = F.cross_entropy(out[dblp_data['author'].train_mask],\n",
    "                                   dblp_data['author'].y[dblp_data['author'].train_mask])\n",
    "            loss.backward()\n",
    "            opt_hgt.step()\n",
    "            return loss.item()\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def test_hgt():\n",
    "            hgt_dblp.eval()\n",
    "            out = hgt_dblp(dblp_data.x_dict, dblp_data.edge_index_dict)\n",
    "            pred = out.argmax(dim=1)\n",
    "            accs = {}\n",
    "            for split in ['train_mask', 'val_mask', 'test_mask']:\n",
    "                mask = dblp_data['author'][split]\n",
    "                acc  = (pred[mask] == dblp_data['author'].y[mask]).float().mean()\n",
    "                accs[split.replace('_mask', '')] = acc.item()\n",
    "            return accs\n",
    "\n",
    "        hgt_losses = []\n",
    "        for epoch in range(1, 101):\n",
    "            l = train_hgt()\n",
    "            hgt_losses.append(l)\n",
    "\n",
    "        accs = test_hgt()\n",
    "        print(f\"Train: {accs['train']:.4f} | Val: {accs['val']:.4f} | Test: {accs['test']:.4f}\")\n",
    "    except NameError:\n",
    "        print('DBLP dataset not loaded — skipping HGT training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Relational GCN (R-GCN)\n",
    "\n",
    "### 4.1 Theory\n",
    "\n",
    "**R-GCN** (Schlichtkrull et al., 2018) extends GCN to **multi-relational (knowledge) graphs** by using **relation-type-specific weight matrices**.\n",
    "\n",
    "#### 4.1.1 Propagation Rule\n",
    "\n",
    "$$\\mathbf{h}_i^{(l+1)} = \\sigma\\!\\left(\n",
    "    \\mathbf{W}_0^{(l)} \\mathbf{h}_i^{(l)}\n",
    "    + \\sum_{r \\in \\mathcal{R}} \\sum_{j \\in \\mathcal{N}_r(i)}\n",
    "      \\frac{1}{c_{i,r}} \\mathbf{W}_r^{(l)} \\mathbf{h}_j^{(l)}\n",
    "\\right)$$\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|----------|\n",
    "| $\\mathbf{W}_0^{(l)}$ | Self-loop weight matrix |\n",
    "| $\\mathbf{W}_r^{(l)}$ | Relation-$r$-specific weight matrix |\n",
    "| $\\mathcal{N}_r(i)$ | Neighbours of $i$ via relation $r$ |\n",
    "| $c_{i,r}$ | Normalisation constant (e.g., $|\\mathcal{N}_r(i)|$) |\n",
    "\n",
    "#### 4.1.2 Parameter Reduction\n",
    "\n",
    "With many relation types, having a separate $\\mathbf{W}_r$ for each $r$ leads to a huge parameter count and overfitting. R-GCN proposes two regularisation techniques:\n",
    "\n",
    "**Basis decomposition:**\n",
    "$$\\mathbf{W}_r = \\sum_{b=1}^B a_{rb} \\mathbf{V}_b$$\n",
    "\n",
    "The relation-specific matrices $\\mathbf{W}_r$ are linear combinations of $B$ shared basis matrices $\\mathbf{V}_b$, with relation-specific coefficients $a_{rb}$.\n",
    "\n",
    "**Block-diagonal decomposition:**\n",
    "$$\\mathbf{W}_r = \\bigoplus_{b=1}^B \\mathbf{Q}_{br}$$\n",
    "\n",
    "Each $\\mathbf{W}_r$ is block-diagonal with blocks $\\mathbf{Q}_{br}$.\n",
    "\n",
    "#### 4.1.3 Applications\n",
    "\n",
    "1. **Entity classification** — classify nodes in a KG\n",
    "2. **Link prediction** — combined with a decoder (e.g., DistMult) for KG completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 R-GCN from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCNConvFromScratch(nn.Module):\n",
    "    \"\"\"\n",
    "    R-GCN convolution layer with basis decomposition.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, num_relations, num_bases=None):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_relations = num_relations\n",
    "        self.num_bases = num_bases or num_relations  # full if None\n",
    "\n",
    "        # Basis matrices\n",
    "        self.basis = nn.Parameter(torch.empty(self.num_bases, in_channels, out_channels))\n",
    "        # Relation-specific coefficients\n",
    "        if num_bases and num_bases < num_relations:\n",
    "            self.att = nn.Parameter(torch.empty(num_relations, self.num_bases))\n",
    "        else:\n",
    "            self.att = None\n",
    "\n",
    "        # Self-loop weight\n",
    "        self.root_weight = nn.Linear(in_channels, out_channels, bias=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_channels))\n",
    "\n",
    "        nn.init.xavier_uniform_(self.basis)\n",
    "        if self.att is not None:\n",
    "            nn.init.xavier_uniform_(self.att)\n",
    "\n",
    "    def weight_for_relation(self, r_idx):\n",
    "        \"\"\"Compute the weight matrix W_r using basis decomposition.\"\"\"\n",
    "        if self.att is not None:\n",
    "            # W_r = sum_b a_{r,b} * V_b\n",
    "            w = (self.att[r_idx].unsqueeze(-1).unsqueeze(-1) * self.basis).sum(0)\n",
    "        else:\n",
    "            w = self.basis[r_idx]\n",
    "        return w   # [in_ch, out_ch]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type):\n",
    "        \"\"\"\n",
    "        x:          [N, in_channels]\n",
    "        edge_index: [2, E]\n",
    "        edge_type:  [E]  integer edge type for each edge\n",
    "        \"\"\"\n",
    "        N = x.size(0)\n",
    "        out = self.root_weight(x)   # self-loop\n",
    "\n",
    "        for r in range(self.num_relations):\n",
    "            mask = edge_type == r\n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "            ei_r = edge_index[:, mask]     # edges of relation r\n",
    "            src, dst = ei_r\n",
    "\n",
    "            W_r = self.weight_for_relation(r)   # [in_ch, out_ch]\n",
    "            msg = x[src] @ W_r                  # [E_r, out_ch]\n",
    "\n",
    "            # Normalise by number of relation-r neighbours\n",
    "            deg_r = torch.zeros(N).scatter_add_(0, dst, torch.ones(dst.size(0)))\n",
    "            norm  = deg_r[dst].clamp(min=1).unsqueeze(-1)\n",
    "            msg   = msg / norm\n",
    "\n",
    "            out.scatter_add_(0, dst.unsqueeze(-1).expand_as(msg), msg)\n",
    "\n",
    "        return F.relu(out + self.bias)\n",
    "\n",
    "\n",
    "print('RGCNConvFromScratch defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a small multi-relational graph\n",
    "N_r = 6\n",
    "num_rel = 3\n",
    "x_r   = torch.randn(N_r, 8)\n",
    "ei_r  = torch.tensor([[0,1,2,3,4,0,2], [1,2,3,4,5,3,5]], dtype=torch.long)\n",
    "et_r  = torch.tensor([0,0,1,1,2,2,0], dtype=torch.long)   # edge types\n",
    "\n",
    "rgcn_scratch = RGCNConvFromScratch(8, 16, num_relations=num_rel, num_bases=2)\n",
    "out_r = rgcn_scratch(x_r, ei_r, et_r)\n",
    "print('RGCN scratch output shape:', out_r.shape)  # [6, 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 R-GCN with PyG's `RGCNConv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyg_available:\n",
    "    from torch_geometric.nn import RGCNConv\n",
    "\n",
    "    class RGCN(nn.Module):\n",
    "        def __init__(self, in_ch, hidden_ch, out_ch, num_relations, num_bases=None):\n",
    "            super().__init__()\n",
    "            self.conv1 = RGCNConv(in_ch, hidden_ch, num_relations=num_relations,\n",
    "                                  num_bases=num_bases)\n",
    "            self.conv2 = RGCNConv(hidden_ch, out_ch, num_relations=num_relations,\n",
    "                                  num_bases=num_bases)\n",
    "\n",
    "        def forward(self, x, edge_index, edge_type):\n",
    "            x = F.relu(self.conv1(x, edge_index, edge_type))\n",
    "            x = F.dropout(x, p=0.3, training=self.training)\n",
    "            x = self.conv2(x, edge_index, edge_type)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "\n",
    "    rgcn_pyg = RGCN(in_ch=8, hidden_ch=16, out_ch=4,\n",
    "                    num_relations=num_rel, num_bases=2)\n",
    "    out_rgcn = rgcn_pyg(x_r, ei_r, et_r)\n",
    "    print('RGCN PyG output shape:', out_rgcn.shape)  # [6, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 R-GCN for Link Prediction on a KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-GCN encoder + DistMult decoder for KG link prediction\n",
    "if pyg_available:\n",
    "    class RGCNLinkPredictor(nn.Module):\n",
    "        \"\"\"\n",
    "        Encoder: R-GCN to produce node embeddings.\n",
    "        Decoder: DistMult scoring function.\n",
    "        \"\"\"\n",
    "        def __init__(self, num_entities, num_relations, hidden_ch, emb_dim, num_bases=None):\n",
    "            super().__init__()\n",
    "            # Learnable entity embeddings as input features\n",
    "            self.entity_emb = nn.Embedding(num_entities, hidden_ch)\n",
    "\n",
    "            # R-GCN encoder\n",
    "            self.encoder = RGCNConv(hidden_ch, emb_dim,\n",
    "                                    num_relations=num_relations,\n",
    "                                    num_bases=num_bases)\n",
    "\n",
    "            # DistMult relation embedding\n",
    "            self.relation_emb = nn.Embedding(num_relations, emb_dim)\n",
    "\n",
    "        def encode(self, edge_index, edge_type):\n",
    "            x = self.entity_emb.weight   # [N, hidden_ch]\n",
    "            z = F.relu(self.encoder(x, edge_index, edge_type))  # [N, emb_dim]\n",
    "            return z\n",
    "\n",
    "        def decode(self, z, h_idx, r_idx, t_idx):\n",
    "            h = z[h_idx]\n",
    "            r = self.relation_emb(r_idx)\n",
    "            t = z[t_idx]\n",
    "            return (h * r * t).sum(dim=-1)\n",
    "\n",
    "        def forward(self, edge_index, edge_type, pos_triples, neg_triples):\n",
    "            z = self.encode(edge_index, edge_type)\n",
    "\n",
    "            ph, pr, pt = pos_triples[:, 0], pos_triples[:, 1], pos_triples[:, 2]\n",
    "            nh, nr, nt = neg_triples[:, 0], neg_triples[:, 1], neg_triples[:, 2]\n",
    "\n",
    "            pos_score = self.decode(z, ph, pr, pt)\n",
    "            neg_score = self.decode(z, nh, nr, nt)\n",
    "\n",
    "            all_scores = torch.cat([pos_score, neg_score])\n",
    "            all_labels = torch.cat([\n",
    "                torch.ones(len(pos_score)),\n",
    "                torch.zeros(len(neg_score))\n",
    "            ])\n",
    "            return F.binary_cross_entropy_with_logits(all_scores, all_labels)\n",
    "\n",
    "    print('RGCNLinkPredictor defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyg_available:\n",
    "    # Use toy KG from Notebook 3\n",
    "    entity2id = {'Alice': 0, 'Bob': 1, 'Carol': 2, 'London': 3, 'Paris': 4, 'UK': 5}\n",
    "    relation2id = {'knows': 0, 'livesIn': 1, 'locatedIn': 2}\n",
    "    triples = [(0,0,1),(1,0,2),(0,1,3),(1,1,4),(3,2,5)]\n",
    "    triples_tensor = torch.tensor(triples, dtype=torch.long)\n",
    "\n",
    "    num_entities_toy  = len(entity2id)\n",
    "    num_relations_toy = len(relation2id)\n",
    "\n",
    "    # Build edge_index and edge_type for R-GCN (use all triples as graph structure)\n",
    "    edge_idx_toy  = triples_tensor[:, [0, 2]].T.contiguous()  # [2, E]\n",
    "    edge_type_toy = triples_tensor[:, 1]                       # [E]\n",
    "\n",
    "    def corrupt_triples_kg(triples, num_ent):\n",
    "        neg = triples.clone()\n",
    "        mask = torch.rand(len(triples)) > 0.5\n",
    "        rand = torch.randint(0, num_ent, (len(triples),))\n",
    "        neg[mask, 0] = rand[mask]\n",
    "        neg[~mask, 2] = rand[~mask]\n",
    "        return neg\n",
    "\n",
    "    rgcn_lp = RGCNLinkPredictor(\n",
    "        num_entities=num_entities_toy,\n",
    "        num_relations=num_relations_toy,\n",
    "        hidden_ch=16, emb_dim=8, num_bases=None\n",
    "    )\n",
    "    opt_rgcn_lp = optim.Adam(rgcn_lp.parameters(), lr=0.01)\n",
    "\n",
    "    lp_losses = []\n",
    "    for epoch in range(300):\n",
    "        neg = corrupt_triples_kg(triples_tensor, num_entities_toy)\n",
    "        opt_rgcn_lp.zero_grad()\n",
    "        loss = rgcn_lp(edge_idx_toy, edge_type_toy, triples_tensor, neg)\n",
    "        loss.backward()\n",
    "        opt_rgcn_lp.step()\n",
    "        lp_losses.append(loss.item())\n",
    "\n",
    "    plt.plot(lp_losses)\n",
    "    plt.xlabel('Epoch'); plt.ylabel('BCE Loss')\n",
    "    plt.title('R-GCN Link Prediction Training Loss'); plt.show()\n",
    "    print(f'Final loss: {lp_losses[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Comparison Experiment\n",
    "\n",
    "### 5.1 Node Classification on Cora: GCN vs Graph Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyg_available:\n",
    "    from torch_geometric.nn import GCNConv\n",
    "\n",
    "    dataset_cora = Planetoid(root='/tmp/Cora', name='Cora',\n",
    "                             transform=T.NormalizeFeatures())\n",
    "    data_cora = dataset_cora[0].to(device)\n",
    "\n",
    "    class GCN_Baseline(nn.Module):\n",
    "        def __init__(self, in_ch, hidden, out_ch):\n",
    "            super().__init__()\n",
    "            self.c1 = GCNConv(in_ch, hidden)\n",
    "            self.c2 = GCNConv(hidden, out_ch)\n",
    "        def forward(self, x, edge_index):\n",
    "            x = F.relu(self.c1(x, edge_index))\n",
    "            x = F.dropout(x, 0.5, training=self.training)\n",
    "            return F.log_softmax(self.c2(x, edge_index), dim=1)\n",
    "\n",
    "    def run_experiment(model_cls, name, **kwargs):\n",
    "        torch.manual_seed(42)\n",
    "        model = model_cls(**kwargs).to(device)\n",
    "        opt   = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "        for _ in range(200):\n",
    "            model.train()\n",
    "            opt.zero_grad()\n",
    "            out = model(data_cora.x, data_cora.edge_index)\n",
    "            F.nll_loss(out[data_cora.train_mask],\n",
    "                       data_cora.y[data_cora.train_mask]).backward()\n",
    "            opt.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out  = model(data_cora.x, data_cora.edge_index)\n",
    "            pred = out.argmax(1)\n",
    "            test_acc = (pred[data_cora.test_mask] ==\n",
    "                        data_cora.y[data_cora.test_mask]).float().mean().item()\n",
    "        print(f'{name}: Test Acc = {test_acc:.4f}')\n",
    "        return test_acc\n",
    "\n",
    "    gcn_acc = run_experiment(\n",
    "        GCN_Baseline, 'GCN',\n",
    "        in_ch=dataset_cora.num_features, hidden=64, out_ch=dataset_cora.num_classes\n",
    "    )\n",
    "\n",
    "    gt_acc = run_experiment(\n",
    "        PyGGraphTransformer, 'Graph Transformer (TransformerConv)',\n",
    "        in_ch=dataset_cora.num_features,\n",
    "        hidden_ch=16, out_ch=dataset_cora.num_classes,\n",
    "        heads=4, num_layers=2, dropout=0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Model Comparison Summary\n",
    "\n",
    "| Model | Node Types | Edge Types | Attention | Long-Range | Scalability |\n",
    "|-------|-----------|-----------|-----------|-----------|-------------|\n",
    "| **GCN** | 1 | 1 | No (degree norm) | No | O(\\|E\\|) |\n",
    "| **Graph Transformer** | 1 | 1 | Yes (MHA) | Yes (global) | O(\\|E\\| or N²) |\n",
    "| **Graphormer** | 1 | 1 | Yes + structural bias | Yes | O(N²) |\n",
    "| **HGT** | Multiple | Multiple | Yes (type-specific) | No (sparse) | O(\\|E\\|) |\n",
    "| **R-GCN** | 1 | Multiple | No | No | O(\\|R\\|·\\|E\\|) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Exercises\n",
    "\n",
    "### Exercise 1 — Positional Encodings for Graph Transformers\n",
    "\n",
    "Laplacian positional encoding (LPE) injects graph-structural information into node features:\n",
    "1. Compute the normalised Laplacian $\\tilde{L}$ for the Cora graph.\n",
    "2. Extract the top-$k$ ($k=8$) non-trivial eigenvectors.\n",
    "3. Concatenate them to node features before passing to the Graph Transformer.\n",
    "4. Compare test accuracy with and without LPE.\n",
    "\n",
    "*Hint: use `scipy.sparse.linalg.eigsh` or `torch.linalg.eigh`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 — your solution here\n",
    "def laplacian_pe(edge_index, num_nodes, k=8):\n",
    "    \"\"\"Compute top-k Laplacian eigenvectors as positional encodings.\"\"\"\n",
    "    # TODO: build normalised Laplacian and compute eigenvectors\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 — Graphormer Centrality Encoding\n",
    "\n",
    "Implement the **centrality encoding** from Graphormer:\n",
    "1. Compute in-degree and out-degree for each node in a directed graph.\n",
    "2. Create learnable degree embeddings.\n",
    "3. Add them to the initial node features.\n",
    "4. Train a simplified Graphormer on a classification task and compare with/without centrality encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 — your solution here\n",
    "class CentralityEncoding(nn.Module):\n",
    "    def __init__(self, max_degree, d_model):\n",
    "        super().__init__()\n",
    "        self.in_deg_emb  = nn.Embedding(max_degree + 1, d_model)\n",
    "        self.out_deg_emb = nn.Embedding(max_degree + 1, d_model)\n",
    "\n",
    "    def forward(self, x, edge_index, num_nodes):\n",
    "        # TODO: compute in/out degrees and add embeddings to x\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 — R-GCN for Entity Classification\n",
    "\n",
    "The **AIFB** and **MUTAG** datasets are popular KG entity classification benchmarks.\n",
    "Using `torch_geometric.datasets.Entities`:\n",
    "1. Load the AIFB dataset.\n",
    "2. Build a 2-layer R-GCN with basis decomposition ($B = 30$).\n",
    "3. Train and report test accuracy.\n",
    "4. Compare the number of parameters with and without basis decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 — your solution here\n",
    "if pyg_available:\n",
    "    try:\n",
    "        from torch_geometric.datasets import Entities\n",
    "        aifb = Entities(root='/tmp/AIFB', name='AIFB')\n",
    "        print(aifb[0])\n",
    "        # TODO: build and train R-GCN\n",
    "    except Exception as e:\n",
    "        print(f'Dataset not available: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 — HGT vs R-GCN on DBLP\n",
    "\n",
    "1. Load the DBLP dataset.\n",
    "2. Convert it to a **homogeneous** graph using `data.to_homogeneous()` and train an R-GCN.\n",
    "3. Train an HGT on the original heterogeneous DBLP graph.\n",
    "4. Compare test accuracy and discuss why HGT should perform better on heterogeneous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 — your solution here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 — Global Attention (Fully Connected) vs Sparse Attention\n",
    "\n",
    "Implement a **fully-connected attention** variant of the Graph Transformer (where every node can attend to every other node, $O(N^2)$):\n",
    "1. Replace the sparse neighbourhood-based attention in `GraphTransformerLayer` with full $N \\times N$ attention.\n",
    "2. Add a binary edge mask to optionally restrict attention to neighbourhoods.\n",
    "3. Run both variants on a small graph, compare attention patterns and performance.\n",
    "4. Discuss the trade-off between expressiveness and scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5 — your solution here\n",
    "class FullAttentionGraphTransformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        # TODO: full N x N attention with optional edge mask\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Model | Core Mechanism | Graph Types | When to Use |\n",
    "|-------|----------------|-------------|-------------|\n",
    "| **Graph Transformer** | MHA + LPE | Homogeneous | Need long-range, interpretable attention |\n",
    "| **Graphormer** | MHA + spatial/centrality/edge biases | Homogeneous | Molecular graphs, high accuracy |\n",
    "| **HGT** | Type-specific MHA | Heterogeneous | Multiple node/edge types with attention |\n",
    "| **R-GCN** | Relation-specific GCN | Multi-relational / KG | Knowledge graphs, entity classification, link prediction |\n",
    "\n",
    "### What We've Covered Across All Notebooks\n",
    "\n",
    "| # | Notebook | Topics |\n",
    "|---|----------|--------|\n",
    "| 1 | PyTorch & PyG | Tensors, autograd, `nn.Module`, `Data`, `MessagePassing` |\n",
    "| 2 | GCN, GraphSAGE, GAT | Spectral GNNs, inductive learning, attention |\n",
    "| 3 | KG Embeddings | TransE, TransR, RotatE, ComplEx, LiteralE, Hetero graphs |\n",
    "| 4 | Advanced GNNs | Graph Transformer, Graphormer, HGT, R-GCN |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
